{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eadad356-bc8b-4ae3-af46-51624e9b6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from os.path import join\n",
    "from os import getcwd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0664c46-861a-429d-a736-14b3d54276ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, auc\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e9ea86-22bd-4c88-8937-c34f49ce6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import AUC # Area under the curve, default: ROC\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.initializers import GlorotNormal\n",
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fec708f-306d-4a12-9d9f-7f8020bb3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4822c966-b64f-4ca0-9604-c9cd4b875624",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = getcwd()+'\\\\checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a024e19-bfc5-45b0-b492-e57323f6183f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../databases/diabetes.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708d29ba-8b33-4740-909e-654070c43da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlayers = {\n",
    "    'BloodPressure': (40, np.Inf),\n",
    "    'SkinThickness': (0, 80),\n",
    "    'Insulin': (0, 400),\n",
    "    'BMI': (0, 50)\n",
    "}\n",
    "\n",
    "zeros = [\n",
    "    'Glucose',\n",
    "    'BloodPressure',\n",
    "    'SkinThickness',\n",
    "    'Insulin',\n",
    "    'BMI'\n",
    "]\n",
    "x_df = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
    "y_df = df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "077675fa-81fe-4dad-b1a2-af4a9384cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_changing_state(x_dataset, y_dataset, model, state, direction, my_callbacks):\n",
    "    # Split dataset into 15% test, 85% train \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_df, y_df, test_size=0.15, random_state=state)\n",
    "    # Reemplazo los valores nulos y los outlayer\n",
    "    x_train_clean, _data = replace_outliers_zeros(x_train, outlayers, zeros, mean_median=True)\n",
    "    x_valid_clean, _data = replace_outliers_zeros(x_valid, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "    # Normalizo los datasets\n",
    "    x_train_norm, _norm_dict = normalize(x_train_clean, None)\n",
    "    x_valid_norm, _norm_dict = normalize(x_valid_clean, _norm_dict)\n",
    "    # Train model\n",
    "    history_mlp_0 = mlp_model.fit(x_train_norm, y_train, validation_data=(x_valid_norm, y_valid),\n",
    "                              batch_size=32, epochs=200,\n",
    "                              verbose=0, callbacks=my_callbacks) \n",
    "    # Cargo el mejor modelo entrenado\n",
    "    mlp_model.load_weights(direction)\n",
    "    results = verify_model(mlp_model, x_train_norm, y_train, x_valid_norm, y_valid)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bab0fcf-e5f0-4578-a471-b4de4003fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into 15% test, 85% train \n",
    "x_temp, x_test, y_temp, y_test = train_test_split(x_df, y_df, test_size=0.15)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_temp, y_temp, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1928f7a-ec8c-45e1-b11f-ff653b4ab78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazo los valores nulos y los outlayer\n",
    "x_train_clean, _data = replace_outliers_zeros(x_train, outlayers, zeros, mean_median=True)\n",
    "x_test_clean, _data = replace_outliers_zeros(x_test, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "x_valid_clean, _data = replace_outliers_zeros(x_valid, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "# Normalizo los datasets\n",
    "x_train_norm, _norm_dict = normalize(x_train_clean, None)\n",
    "x_valid_norm, _norm_dict = normalize(x_valid_clean, _norm_dict)\n",
    "x_test_norm, _norm_dict = normalize(x_test_clean, _norm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc8e915-c772-4560-a6f5-434ecee7a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = EarlyStopping(monitor='val_auc', patience=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ce718-b28b-4d57-a0d7-3a99e8d02098",
   "metadata": {},
   "source": [
    "### Definici√≥n de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0135178c-9c0a-44c1-b1dc-4a3bfbd2f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlp_0\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_0_checkpoint_callback = ModelCheckpoint(filepath=join(checkpoints_path, 'mlp_0'), save_weights_only=True, monitor='val_auc', mode='max', \n",
    "                                           save_best_only=True)\n",
    "mlp_model = Sequential(name='mlp_0')\n",
    "\n",
    "mlp_model.add(Dense(10, activation='relu', input_shape=(x_train_norm.shape[1],)))\n",
    "mlp_model.add(Dense(5, activation='linear'))\n",
    "mlp_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5efacb4-e902-4b98-8aca-42089e2a1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c4e0371-ae2c-4392-9e57-c3ec22564c62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.7658 - auc: 0.6722 - val_loss: 0.6639 - val_auc: 0.7778\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.5533 - auc: 0.8291 - val_loss: 0.5745 - val_auc: 0.8180\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4968 - auc: 0.8476 - val_loss: 0.5371 - val_auc: 0.8214\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.4772 - auc: 0.8535 - val_loss: 0.5212 - val_auc: 0.8276\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4631 - auc: 0.8590 - val_loss: 0.5249 - val_auc: 0.8293\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.4582 - auc: 0.8612 - val_loss: 0.5164 - val_auc: 0.8329\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4508 - auc: 0.8620 - val_loss: 0.5311 - val_auc: 0.8340\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.4516 - auc: 0.8617 - val_loss: 0.5202 - val_auc: 0.8395\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4449 - auc: 0.8680 - val_loss: 0.5052 - val_auc: 0.8301\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4450 - auc: 0.8641 - val_loss: 0.5342 - val_auc: 0.8316\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4399 - auc: 0.8694 - val_loss: 0.5033 - val_auc: 0.8359\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4506 - auc: 0.8649 - val_loss: 0.5047 - val_auc: 0.8331\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4429 - auc: 0.8687 - val_loss: 0.5078 - val_auc: 0.8304\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4351 - auc: 0.8735 - val_loss: 0.5225 - val_auc: 0.8301\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4280 - auc: 0.8775 - val_loss: 0.5068 - val_auc: 0.8301\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4339 - auc: 0.8748 - val_loss: 0.5057 - val_auc: 0.8329\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4242 - auc: 0.8793 - val_loss: 0.5079 - val_auc: 0.8287\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4254 - auc: 0.8783 - val_loss: 0.5277 - val_auc: 0.8267\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4209 - auc: 0.8814 - val_loss: 0.5096 - val_auc: 0.8327\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4164 - auc: 0.8839 - val_loss: 0.5120 - val_auc: 0.8312\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4155 - auc: 0.8834 - val_loss: 0.5057 - val_auc: 0.8329\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4227 - auc: 0.8819 - val_loss: 0.4934 - val_auc: 0.8376\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4383 - auc: 0.8736 - val_loss: 0.5042 - val_auc: 0.8340\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4332 - auc: 0.8770 - val_loss: 0.5603 - val_auc: 0.8310\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4142 - auc: 0.8885 - val_loss: 0.5040 - val_auc: 0.8365\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4093 - auc: 0.8883 - val_loss: 0.5179 - val_auc: 0.8333\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4066 - auc: 0.8905 - val_loss: 0.5125 - val_auc: 0.8327\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4095 - auc: 0.8873 - val_loss: 0.5132 - val_auc: 0.8348\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4069 - auc: 0.8878 - val_loss: 0.5163 - val_auc: 0.8276\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4031 - auc: 0.8906 - val_loss: 0.5184 - val_auc: 0.8291\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4104 - auc: 0.8873 - val_loss: 0.5445 - val_auc: 0.8316\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4075 - auc: 0.8886 - val_loss: 0.5523 - val_auc: 0.8301\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4091 - auc: 0.8916 - val_loss: 0.5048 - val_auc: 0.8293\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3990 - auc: 0.8951 - val_loss: 0.5704 - val_auc: 0.8331\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4062 - auc: 0.8903 - val_loss: 0.5126 - val_auc: 0.8299\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4123 - auc: 0.8882 - val_loss: 0.5243 - val_auc: 0.8327\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4102 - auc: 0.8891 - val_loss: 0.5998 - val_auc: 0.8314\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4076 - auc: 0.8899 - val_loss: 0.5271 - val_auc: 0.8272\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3958 - auc: 0.8964 - val_loss: 0.5462 - val_auc: 0.8331\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3991 - auc: 0.8947 - val_loss: 0.5272 - val_auc: 0.8325\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3950 - auc: 0.8973 - val_loss: 0.5235 - val_auc: 0.8306\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3929 - auc: 0.8984 - val_loss: 0.5378 - val_auc: 0.8314\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3935 - auc: 0.8971 - val_loss: 0.5136 - val_auc: 0.8327\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3987 - auc: 0.8968 - val_loss: 0.5478 - val_auc: 0.8335\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3985 - auc: 0.8935 - val_loss: 0.5267 - val_auc: 0.8278\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3962 - auc: 0.8950 - val_loss: 0.5128 - val_auc: 0.8312\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3969 - auc: 0.8959 - val_loss: 0.5310 - val_auc: 0.8274\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3913 - auc: 0.8969 - val_loss: 0.5448 - val_auc: 0.8291\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3912 - auc: 0.8967 - val_loss: 0.5218 - val_auc: 0.8250\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3932 - auc: 0.8963 - val_loss: 0.5236 - val_auc: 0.8248\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3946 - auc: 0.8959 - val_loss: 0.5351 - val_auc: 0.8255\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3876 - auc: 0.8999 - val_loss: 0.5379 - val_auc: 0.8291\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3898 - auc: 0.8985 - val_loss: 0.5286 - val_auc: 0.8291\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3940 - auc: 0.8974 - val_loss: 0.5289 - val_auc: 0.8295\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4061 - auc: 0.8912 - val_loss: 0.5166 - val_auc: 0.8236\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3976 - auc: 0.8981 - val_loss: 0.5275 - val_auc: 0.8240\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3916 - auc: 0.9003 - val_loss: 0.5580 - val_auc: 0.8276\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3998 - auc: 0.8936 - val_loss: 0.5645 - val_auc: 0.8257\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3876 - auc: 0.8997 - val_loss: 0.5366 - val_auc: 0.8248\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3847 - auc: 0.9012 - val_loss: 0.5554 - val_auc: 0.8238\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3844 - auc: 0.9011 - val_loss: 0.5482 - val_auc: 0.8229\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3868 - auc: 0.9001 - val_loss: 0.5624 - val_auc: 0.8242\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3888 - auc: 0.8988 - val_loss: 0.5339 - val_auc: 0.8227\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3874 - auc: 0.9005 - val_loss: 0.5184 - val_auc: 0.8257\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3955 - auc: 0.8951 - val_loss: 0.5249 - val_auc: 0.8170\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3964 - auc: 0.8969 - val_loss: 0.5430 - val_auc: 0.8208\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3866 - auc: 0.9008 - val_loss: 0.5341 - val_auc: 0.8295\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4237 - auc: 0.906 - 0s 2ms/step - loss: 0.3942 - auc: 0.8992 - val_loss: 0.5438 - val_auc: 0.8321\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3870 - auc: 0.9015 - val_loss: 0.5670 - val_auc: 0.8306\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3829 - auc: 0.9019 - val_loss: 0.5210 - val_auc: 0.8236\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3827 - auc: 0.9033 - val_loss: 0.5670 - val_auc: 0.8231\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3831 - auc: 0.9016 - val_loss: 0.5371 - val_auc: 0.8227\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3869 - auc: 0.9010 - val_loss: 0.5246 - val_auc: 0.8244\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3917 - auc: 0.8992 - val_loss: 0.5352 - val_auc: 0.8246\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3799 - auc: 0.9041 - val_loss: 0.5862 - val_auc: 0.8223\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3831 - auc: 0.9026 - val_loss: 0.5670 - val_auc: 0.8210\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3818 - auc: 0.9027 - val_loss: 0.5437 - val_auc: 0.8231\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3784 - auc: 0.9049 - val_loss: 0.5758 - val_auc: 0.8265\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3837 - auc: 0.9017 - val_loss: 0.5781 - val_auc: 0.8250\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3864 - auc: 0.9018 - val_loss: 0.5210 - val_auc: 0.8212\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3901 - auc: 0.8985 - val_loss: 0.5491 - val_auc: 0.8240\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3926 - auc: 0.8982 - val_loss: 0.5836 - val_auc: 0.8216\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3853 - auc: 0.9025 - val_loss: 0.5289 - val_auc: 0.8261\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3887 - auc: 0.8979 - val_loss: 0.5263 - val_auc: 0.8236\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3825 - auc: 0.9022 - val_loss: 0.6028 - val_auc: 0.8267\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3828 - auc: 0.9016 - val_loss: 0.5681 - val_auc: 0.8212\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3796 - auc: 0.9038 - val_loss: 0.5405 - val_auc: 0.8240\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3832 - auc: 0.9028 - val_loss: 0.5419 - val_auc: 0.8253\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3846 - auc: 0.9016 - val_loss: 0.5353 - val_auc: 0.8214\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3803 - auc: 0.9044 - val_loss: 0.5957 - val_auc: 0.8193\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3804 - auc: 0.9035 - val_loss: 0.6027 - val_auc: 0.8214\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3843 - auc: 0.9022 - val_loss: 0.5121 - val_auc: 0.8265\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3858 - auc: 0.9037 - val_loss: 0.5952 - val_auc: 0.8219\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3793 - auc: 0.9030 - val_loss: 0.5691 - val_auc: 0.8246\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3822 - auc: 0.9009 - val_loss: 0.5246 - val_auc: 0.8259\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3842 - auc: 0.9018 - val_loss: 0.5671 - val_auc: 0.8259\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3774 - auc: 0.9053 - val_loss: 0.5386 - val_auc: 0.8208\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3771 - auc: 0.9051 - val_loss: 0.6191 - val_auc: 0.8225\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3899 - auc: 0.9003 - val_loss: 0.5511 - val_auc: 0.8270\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3858 - auc: 0.9011 - val_loss: 0.5273 - val_auc: 0.8253\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3809 - auc: 0.9029 - val_loss: 0.5311 - val_auc: 0.8225\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3784 - auc: 0.9038 - val_loss: 0.5824 - val_auc: 0.8280\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3858 - auc: 0.9007 - val_loss: 0.5701 - val_auc: 0.8242\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3796 - auc: 0.9038 - val_loss: 0.5290 - val_auc: 0.8238\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3792 - auc: 0.9033 - val_loss: 0.5626 - val_auc: 0.8206\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3948 - auc: 0.8957 - val_loss: 0.5441 - val_auc: 0.8180\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3780 - auc: 0.9047 - val_loss: 0.5601 - val_auc: 0.8231\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3824 - auc: 0.9040 - val_loss: 0.5214 - val_auc: 0.8267\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3742 - auc: 0.9065 - val_loss: 0.6077 - val_auc: 0.8227\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3778 - auc: 0.9048 - val_loss: 0.5608 - val_auc: 0.8229\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3737 - auc: 0.9062 - val_loss: 0.5928 - val_auc: 0.8229\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3795 - auc: 0.9040 - val_loss: 0.5605 - val_auc: 0.8193\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3738 - auc: 0.9068 - val_loss: 0.5461 - val_auc: 0.8182\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3777 - auc: 0.9053 - val_loss: 0.5578 - val_auc: 0.8208\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3747 - auc: 0.9069 - val_loss: 0.6029 - val_auc: 0.8219\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3811 - auc: 0.9029 - val_loss: 0.5661 - val_auc: 0.8236\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3790 - auc: 0.9049 - val_loss: 0.5193 - val_auc: 0.8314\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3854 - auc: 0.9008 - val_loss: 0.5210 - val_auc: 0.8270\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3847 - auc: 0.9030 - val_loss: 0.6327 - val_auc: 0.8212\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3804 - auc: 0.9045 - val_loss: 0.5702 - val_auc: 0.8121\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3885 - auc: 0.9043 - val_loss: 0.5289 - val_auc: 0.8240\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3845 - auc: 0.9035 - val_loss: 0.7012 - val_auc: 0.8153\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3836 - auc: 0.9026 - val_loss: 0.5519 - val_auc: 0.8114\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3746 - auc: 0.9067 - val_loss: 0.5833 - val_auc: 0.8144\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3741 - auc: 0.9087 - val_loss: 0.5851 - val_auc: 0.8136\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3796 - auc: 0.9049 - val_loss: 0.5533 - val_auc: 0.8157\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3829 - auc: 0.9020 - val_loss: 0.5686 - val_auc: 0.8191\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3839 - auc: 0.9028 - val_loss: 0.6121 - val_auc: 0.8212\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3863 - auc: 0.9047 - val_loss: 0.5470 - val_auc: 0.8242\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3749 - auc: 0.9069 - val_loss: 0.5811 - val_auc: 0.8148\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3752 - auc: 0.9079 - val_loss: 0.5443 - val_auc: 0.8165\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3776 - auc: 0.9061 - val_loss: 0.6006 - val_auc: 0.8134\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3761 - auc: 0.9055 - val_loss: 0.5660 - val_auc: 0.8123\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3763 - auc: 0.9067 - val_loss: 0.5389 - val_auc: 0.8161\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3732 - auc: 0.9076 - val_loss: 0.5666 - val_auc: 0.8157\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3717 - auc: 0.9080 - val_loss: 0.5764 - val_auc: 0.8197\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3713 - auc: 0.9089 - val_loss: 0.5443 - val_auc: 0.8151\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4051 - auc: 0.908 - 0s 2ms/step - loss: 0.3749 - auc: 0.9051 - val_loss: 0.5695 - val_auc: 0.8185\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3747 - auc: 0.9064 - val_loss: 0.6582 - val_auc: 0.8197\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3790 - auc: 0.9073 - val_loss: 0.5777 - val_auc: 0.8129\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3731 - auc: 0.9097 - val_loss: 0.5444 - val_auc: 0.8170\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3792 - auc: 0.9030 - val_loss: 0.5662 - val_auc: 0.8178\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3708 - auc: 0.9091 - val_loss: 0.5376 - val_auc: 0.8189\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3742 - auc: 0.9079 - val_loss: 0.6167 - val_auc: 0.8138\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3747 - auc: 0.9079 - val_loss: 0.5780 - val_auc: 0.8204\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3684 - auc: 0.9109 - val_loss: 0.5845 - val_auc: 0.8157\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3717 - auc: 0.9083 - val_loss: 0.5434 - val_auc: 0.8168\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3906 - auc: 0.8990 - val_loss: 0.6471 - val_auc: 0.8119\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3703 - auc: 0.9100 - val_loss: 0.5852 - val_auc: 0.8040\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3706 - auc: 0.9096 - val_loss: 0.5964 - val_auc: 0.8031\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3747 - auc: 0.9082 - val_loss: 0.5531 - val_auc: 0.8089\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3746 - auc: 0.9133 - val_loss: 0.6838 - val_auc: 0.8102\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3862 - auc: 0.9080 - val_loss: 0.5457 - val_auc: 0.8095\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3823 - auc: 0.9036 - val_loss: 0.6030 - val_auc: 0.8068\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3697 - auc: 0.9102 - val_loss: 0.6140 - val_auc: 0.8121\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3680 - auc: 0.9105 - val_loss: 0.5894 - val_auc: 0.8059\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3659 - auc: 0.9130 - val_loss: 0.6060 - val_auc: 0.8099\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3759 - auc: 0.9070 - val_loss: 0.6380 - val_auc: 0.8085\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3747 - auc: 0.9099 - val_loss: 0.5695 - val_auc: 0.8061\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3667 - auc: 0.9105 - val_loss: 0.6119 - val_auc: 0.8119\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3665 - auc: 0.9112 - val_loss: 0.5753 - val_auc: 0.8055\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3674 - auc: 0.9100 - val_loss: 0.5653 - val_auc: 0.8093\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3684 - auc: 0.9094 - val_loss: 0.6177 - val_auc: 0.8042\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3685 - auc: 0.9098 - val_loss: 0.5947 - val_auc: 0.8010\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3756 - auc: 0.9068 - val_loss: 0.6863 - val_auc: 0.8059\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3717 - auc: 0.9098 - val_loss: 0.5697 - val_auc: 0.8017\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3801 - auc: 0.9066 - val_loss: 0.6214 - val_auc: 0.8044\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3659 - auc: 0.9123 - val_loss: 0.6405 - val_auc: 0.8055\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3658 - auc: 0.9123 - val_loss: 0.5902 - val_auc: 0.8070\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3612 - auc: 0.9132 - val_loss: 0.6352 - val_auc: 0.7995\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3624 - auc: 0.9147 - val_loss: 0.5665 - val_auc: 0.8063\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3617 - auc: 0.9138 - val_loss: 0.6155 - val_auc: 0.8059\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3622 - auc: 0.9133 - val_loss: 0.5957 - val_auc: 0.8038\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3616 - auc: 0.9155 - val_loss: 0.6190 - val_auc: 0.8112\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3615 - auc: 0.9133 - val_loss: 0.5872 - val_auc: 0.8053\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3652 - auc: 0.9124 - val_loss: 0.5651 - val_auc: 0.8014\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3640 - auc: 0.9132 - val_loss: 0.6485 - val_auc: 0.8012\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3629 - auc: 0.9115 - val_loss: 0.5673 - val_auc: 0.8106\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3579 - auc: 0.9152 - val_loss: 0.6293 - val_auc: 0.8102\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3620 - auc: 0.9138 - val_loss: 0.5769 - val_auc: 0.8061\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3593 - auc: 0.9133 - val_loss: 0.6176 - val_auc: 0.8046\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3581 - auc: 0.9156 - val_loss: 0.6339 - val_auc: 0.8023\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3698 - auc: 0.9097 - val_loss: 0.6201 - val_auc: 0.7997\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3705 - auc: 0.9098 - val_loss: 0.6739 - val_auc: 0.7938\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3629 - auc: 0.9120 - val_loss: 0.6002 - val_auc: 0.8012\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3585 - auc: 0.9147 - val_loss: 0.6395 - val_auc: 0.8040\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3652 - auc: 0.9104 - val_loss: 0.6143 - val_auc: 0.8029\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3612 - auc: 0.9141 - val_loss: 0.5916 - val_auc: 0.7944\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3618 - auc: 0.9126 - val_loss: 0.5907 - val_auc: 0.8006\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3602 - auc: 0.9145 - val_loss: 0.5984 - val_auc: 0.7961\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3603 - auc: 0.9141 - val_loss: 0.7196 - val_auc: 0.8014\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3637 - auc: 0.9143 - val_loss: 0.6245 - val_auc: 0.7923\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3589 - auc: 0.9156 - val_loss: 0.6068 - val_auc: 0.7942\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3537 - auc: 0.9171 - val_loss: 0.6493 - val_auc: 0.7921\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3616 - auc: 0.9127 - val_loss: 0.6418 - val_auc: 0.7976\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3598 - auc: 0.9125 - val_loss: 0.6297 - val_auc: 0.7997\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3528 - auc: 0.9167 - val_loss: 0.6443 - val_auc: 0.7966\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3675 - auc: 0.9112 - val_loss: 0.5750 - val_auc: 0.7910\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3657 - auc: 0.9135 - val_loss: 0.6224 - val_auc: 0.7951\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3700 - auc: 0.9126 - val_loss: 0.7360 - val_auc: 0.7921\n"
     ]
    }
   ],
   "source": [
    "history_mlp_0 = mlp_model.fit(x_train_norm, y_train, validation_data=(x_valid_norm, y_valid),\n",
    "                              batch_size=32, epochs=200,\n",
    "                              verbose=1, callbacks=[mlp_0_checkpoint_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2468623-f729-4536-b0aa-0376dec4f190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set</th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Especificidad</th>\n",
       "      <th>Sensibilidad</th>\n",
       "      <th>Valor Predictivo Positivo</th>\n",
       "      <th>Valor Predictivo Negativo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>0.868713</td>\n",
       "      <td>0.883978</td>\n",
       "      <td>0.619792</td>\n",
       "      <td>0.73913</td>\n",
       "      <td>0.814249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Validacion</td>\n",
       "      <td>0.838861</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.742424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Set   AUC ROC  Especificidad  Sensibilidad  \\\n",
       "0       Train  0.868713       0.883978      0.619792   \n",
       "1  Validacion  0.838861       0.875000      0.595238   \n",
       "\n",
       "   Valor Predictivo Positivo  Valor Predictivo Negativo  \n",
       "0                    0.73913                   0.814249  \n",
       "1                    0.78125                   0.742424  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargo el mejor modelo entrenado\n",
    "mlp_model.load_weights(join(checkpoints_path, 'mlp_0'))\n",
    "verify_model(mlp_model, x_train_norm, y_train, x_valid_norm, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d906e-cc88-413d-a538-acf90b7fc195",
   "metadata": {},
   "source": [
    "Salieron resultados interesantes, vamos a verificar si se mantiene al cambiar el random state al hacer split de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e491747-9265-4e72-9cbe-de10788cbfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlp_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_state = Sequential(name='mlp_1')\n",
    "mlp_state.add(Dense(10, activation='relu', input_shape=(x_train_norm.shape[1],)))\n",
    "mlp_state.add(Dense(5, activation='linear'))\n",
    "mlp_state.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "mlp_state.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "pesos_inicial = mlp_state.get_weights()\n",
    "mlp_state.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c79987f8-7c3f-4504-9e03-1bfed5c67d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing state 0\n",
      "Runing state 1\n",
      "Runing state 2\n",
      "Runing state 3\n",
      "Runing state 4\n",
      "Runing state 5\n",
      "Runing state 6\n",
      "Runing state 7\n",
      "Runing state 8\n",
      "Runing state 9\n"
     ]
    }
   ],
   "source": [
    "train_auc = []\n",
    "valid_auc = []\n",
    "results = []\n",
    "\n",
    "for i in range(10):\n",
    "    print('Runing state', i)\n",
    "    name  = 'mlp_state_{}'.format(i)\n",
    "    mlp_state.set_weights(pesos_inicial)\n",
    "    checkdir = join(checkpoints_path, name)\n",
    "    temp_callback = ModelCheckpoint(filepath=checkdir, save_weights_only=True, monitor='val_auc', mode='max', save_best_only=True)\n",
    "    metrics = run_changing_state(x_temp, y_temp, mlp_state, i, checkdir, [temp_callback])\n",
    "    results.append(metrics)\n",
    "    train_auc.append(metrics['AUC ROC'][0])\n",
    "    valid_auc.append(metrics['AUC ROC'][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cdace0c-54bb-411b-93c1-4c1111d267d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_auc, label='Train')\n",
    "plt.plot(valid_auc, label='Validation')\n",
    "mean = np.mean(valid_auc)\n",
    "std = np.std(valid_auc)\n",
    "plt.hlines([mean+std, mean, mean-std], 0, 9)   \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc3079-2d04-4d44-b345-c8a8be95bf84",
   "metadata": {},
   "source": [
    "Se puede observar que en la mayoria de los casos la metrica de validaci√≥n da mejor que la metrica de train, con lo cual se puede inferir que el modelo tendr√° una gran varianza debido a la poca cantidad de datos disponible para los par√°metros que requiere entrenar el modelo. Por lo tanto una mejor opcion es utilizar k-folding en lugar de hold out validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901419e-6962-4b1b-9ed4-a1a2acfa0e54",
   "metadata": {},
   "source": [
    "Ahora vamos a probar utilizar 1 o 2 capas ocultas, variando la cantidad de neuronas entre 5, 10 y 30, modificando la activacion entre relu y lineal. Para ello utilizaremos la tecnica de k-folding y la metrica de cada set de hyperpar√°metros sera el promedio de las metricas de cada fold. Una vez testeado esto se continuara buscando mejorar el modelo cambiando la regularizacion o agregando capas de Droput o Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32deaee6-b847-4210-9f3d-944edec57fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folding = KFold(n_splits=5)\n",
    "\n",
    "def train_model(x_dataset, y_dataset, model, name, batch, stopping_patiece=None):\n",
    "    pesos_default = model.get_weights()\n",
    "    folds = folding.split(x_dataset)\n",
    "    auc_val = []\n",
    "    results = []\n",
    "    curr = 0\n",
    "    for train, valid in folds:\n",
    "        # Split dataset\n",
    "        x_train, x_valid = x_dataset.iloc[train], x_dataset.iloc[valid]\n",
    "        y_train, y_valid = y_dataset.iloc[train], y_dataset.iloc[valid]\n",
    "        \n",
    "        # Proceso los datos\n",
    "        x_train_c, _data = replace_outliers_zeros(x_train, outlayers, zeros, mean_median=True)\n",
    "        x_valid_c, _data = replace_outliers_zeros(x_valid, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "        # Normalizo los datasets\n",
    "        x_train_n, _norm_dict = normalize(x_train_c, None)\n",
    "        x_valid_n, _norm_dict = normalize(x_valid_c, _norm_dict)\n",
    "    \n",
    "        # Create callbacks\n",
    "        checkdir = join(checkpoints_path, name+'f{}'.format(curr))\n",
    "        temp_callback = ModelCheckpoint(filepath=checkdir, save_weights_only=True, monitor='val_auc', mode='max', save_best_only=True)\n",
    "        my_callbacks = [temp_callback]\n",
    "        if stopping_patiece:\n",
    "            my_callbacks.append(EarlyStopping(monitor='val_auc', patience=stopping_patiece))\n",
    "        # Train model\n",
    "        history = model.fit(x_train_n, y_train, validation_data=(x_valid_n, y_valid),\n",
    "                            batch_size=batch, epochs=200,\n",
    "                            verbose=0, callbacks=my_callbacks) \n",
    "        # Cargo el mejor modelo entrenado\n",
    "        model.load_weights(checkdir)\n",
    "        metrics = verify_model(model, x_train_n, y_train, x_valid_n, y_valid)\n",
    "        auc_val.append(metrics['AUC ROC'][1])\n",
    "        results.append(metrics)\n",
    "        # Reset model for next training\n",
    "        model.set_weights(pesos_default)\n",
    "        curr += 1\n",
    "    return np.mean(auc_val), results    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2080a7-310c-4822-8a1f-ed6c835c5dc3",
   "metadata": {},
   "source": [
    "1 capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "633e8759-b00f-4a5b-88c2-7606855946a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing -> n5_b32_linear\n",
      "Auc score =  0.8489222305662751\n",
      "Testing -> n5_b64_linear\n",
      "Auc score =  0.8495369591980284\n",
      "Testing -> n5_b32_relu\n",
      "Auc score =  0.8473222668048805\n",
      "Testing -> n5_b64_relu\n",
      "Auc score =  0.8514155678242801\n",
      "Testing -> n10_b32_linear\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "Auc score =  0.8525526440588657\n",
      "Testing -> n10_b64_linear\n",
      "Auc score =  0.8509961448981256\n",
      "Testing -> n10_b32_relu\n",
      "Auc score =  0.8525172947877447\n",
      "Testing -> n10_b64_relu\n",
      "Auc score =  0.8545939107331251\n",
      "Testing -> n30_b32_linear\n",
      "Auc score =  0.8515067784299882\n",
      "Testing -> n30_b64_linear\n",
      "Auc score =  0.8524832785145204\n",
      "Testing -> n30_b32_relu\n",
      "Auc score =  0.8619362085476718\n",
      "Testing -> n30_b64_relu\n",
      "Auc score =  0.8572797936260488\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for neuronas in [5, 10, 30]:\n",
    "    for activation in ['linear', 'relu']:\n",
    "        for batch in [32, 64]:\n",
    "            name = 'n{}_b{}_{}'.format(neuronas, batch, activation)\n",
    "            print('Testing ->', name)\n",
    "            # Armo el modelo\n",
    "            test_model = Sequential(name=name)\n",
    "            test_model.add(Dense(neuronas, activation=activation, input_shape=(x_temp.shape[1],)))\n",
    "            test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "            # Compilo el modelo\n",
    "            test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "            auc, metrics = train_model(x_temp, y_temp, test_model, name, batch, stopping_patiece=50)\n",
    "            results[name] = (auc, metrics)\n",
    "            print('Auc score = ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b522c119-5a74-4aaf-b315-63453017dd68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing -> na5_nb5_b32_linear\n",
      "Testing -> na5_nb5_b64_linear\n",
      "Testing -> na5_nb5_b32_relu\n",
      "Testing -> na5_nb5_b64_relu\n",
      "Testing -> na5_nb10_b32_linear\n",
      "Testing -> na5_nb10_b64_linear\n",
      "Testing -> na5_nb10_b32_relu\n",
      "Testing -> na5_nb10_b64_relu\n",
      "Testing -> na5_nb30_b32_linear\n",
      "Testing -> na5_nb30_b64_linear\n",
      "Testing -> na5_nb30_b32_relu\n",
      "Testing -> na5_nb30_b64_relu\n",
      "Testing -> na10_nb5_b32_linear\n",
      "Testing -> na10_nb5_b64_linear\n",
      "Testing -> na10_nb5_b32_relu\n",
      "Testing -> na10_nb5_b64_relu\n",
      "Testing -> na10_nb10_b32_linear\n",
      "Testing -> na10_nb10_b64_linear\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0081s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0156s). Check your callbacks.\n",
      "Testing -> na10_nb10_b32_relu\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0001s). Check your callbacks.\n",
      "Testing -> na10_nb10_b64_relu\n",
      "Testing -> na10_nb30_b32_linear\n",
      "Testing -> na10_nb30_b64_linear\n",
      "Testing -> na10_nb30_b32_relu\n",
      "Testing -> na10_nb30_b64_relu\n",
      "Testing -> na30_nb5_b32_linear\n",
      "Testing -> na30_nb5_b64_linear\n",
      "Testing -> na30_nb5_b32_relu\n",
      "Testing -> na30_nb5_b64_relu\n",
      "Testing -> na30_nb10_b32_linear\n",
      "Testing -> na30_nb10_b64_linear\n",
      "Testing -> na30_nb10_b32_relu\n",
      "Testing -> na30_nb10_b64_relu\n",
      "Testing -> na30_nb30_b32_linear\n",
      "Testing -> na30_nb30_b64_linear\n",
      "Testing -> na30_nb30_b32_relu\n",
      "Testing -> na30_nb30_b64_relu\n"
     ]
    }
   ],
   "source": [
    "for neuronas_1 in [5, 10, 30]:\n",
    "    for neuronas_2 in [5, 10, 30]:\n",
    "        for activation in ['linear', 'relu']:\n",
    "            for batch in [32, 64]:\n",
    "                name = 'na{}_nb{}_b{}_{}'.format(neuronas_1, neuronas_2, batch, activation)\n",
    "                print('Testing ->', name)\n",
    "                # Armo el modelo\n",
    "                test_model = Sequential(name=name)\n",
    "                test_model.add(Dense(neuronas_1, activation=activation, input_shape=(x_temp.shape[1],)))\n",
    "                test_model.add(Dense(neuronas_2, activation=activation))\n",
    "                test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "                # Compilo el modelo\n",
    "                test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "                auc, metrics = train_model(x_temp, y_temp, test_model, name, batch, stopping_patiece=50)\n",
    "                results[name] = (auc, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe383c-2854-4ae0-8838-738747eedd59",
   "metadata": {},
   "source": [
    "Veo los 10 mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91b488f9-fcef-4761-b17c-a38956166f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "na10_nb10_b32_relu -> 0.8641499719995445\n",
      "na5_nb30_b32_relu -> 0.8632982329595867\n",
      "na5_nb5_b64_relu -> 0.8623856381004117\n",
      "na10_nb30_b64_relu -> 0.8623032403796616\n",
      "na5_nb5_b32_relu -> 0.8621813101845224\n",
      "n30_b32_relu -> 0.8619362085476718\n",
      "na30_nb30_b64_relu -> 0.8610279454687673\n",
      "na10_nb5_b32_relu -> 0.8606602986492125\n",
      "na10_nb30_b32_relu -> 0.8606254855820072\n",
      "na30_nb10_b32_relu -> 0.8597991450868422\n"
     ]
    }
   ],
   "source": [
    "sorted_results = list(reversed(sorted(results.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results[0:10]:\n",
    "    print(i[0], '->', i[1][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c6fbf-5ebd-4482-84d8-8760d599e0e5",
   "metadata": {},
   "source": [
    "Se puede observar como los de mejor performance son en su mayoria redes con 2 capas ocultas, y con gran cantidad de neuronas en cada una. Con estos resultados se buscara optimizar los hyperpar√°metros de los 3 mejores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26a31e61-2ea6-4415-807f-312dd67f09ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing -> na30_nb10_b32_relu_relu_d0\n",
      "Testing -> na30_nb10_b32_relu_relu_d1\n",
      "Testing -> na30_nb10_b64_relu_relu_d0\n",
      "Testing -> na30_nb10_b64_relu_relu_d1\n",
      "Testing -> na30_nb10_b32_relu_linear_d0\n",
      "Testing -> na30_nb10_b32_relu_linear_d1\n",
      "Testing -> na30_nb10_b64_relu_linear_d0\n",
      "Testing -> na30_nb10_b64_relu_linear_d1\n",
      "Testing -> na30_nb10_b32_linear_relu_d0\n",
      "Testing -> na30_nb10_b32_linear_relu_d1\n",
      "Testing -> na30_nb10_b64_linear_relu_d0\n",
      "Testing -> na30_nb10_b64_linear_relu_d1\n",
      "Testing -> na30_nb10_b32_linear_linear_d0\n",
      "Testing -> na30_nb10_b32_linear_linear_d1\n",
      "Testing -> na30_nb10_b64_linear_linear_d0\n",
      "Testing -> na30_nb10_b64_linear_linear_d1\n",
      "Testing -> na30_nb30_b32_relu_relu_d0\n",
      "Testing -> na30_nb30_b32_relu_relu_d1\n",
      "Testing -> na30_nb30_b64_relu_relu_d0\n",
      "Testing -> na30_nb30_b64_relu_relu_d1\n",
      "Testing -> na30_nb30_b32_relu_linear_d0\n",
      "Testing -> na30_nb30_b32_relu_linear_d1\n",
      "Testing -> na30_nb30_b64_relu_linear_d0\n",
      "Testing -> na30_nb30_b64_relu_linear_d1\n",
      "Testing -> na30_nb30_b32_linear_relu_d0\n",
      "Testing -> na30_nb30_b32_linear_relu_d1\n",
      "Testing -> na30_nb30_b64_linear_relu_d0\n",
      "Testing -> na30_nb30_b64_linear_relu_d1\n",
      "Testing -> na30_nb30_b32_linear_linear_d0\n",
      "Testing -> na30_nb30_b32_linear_linear_d1\n",
      "Testing -> na30_nb30_b64_linear_linear_d0\n",
      "Testing -> na30_nb30_b64_linear_linear_d1\n",
      "na30_nb30_b64_linear_relu_d1 -> 0.8718456409958726\n",
      "na30_nb30_b32_linear_relu_d1 -> 0.8709785182197487\n",
      "na30_nb30_b64_linear_relu_d0 -> 0.8691596502315699\n",
      "na30_nb30_b32_linear_relu_d0 -> 0.868947290262135\n",
      "na30_nb10_b32_linear_relu_d1 -> 0.8688784979878672\n",
      "na30_nb10_b64_linear_relu_d1 -> 0.8673516079479417\n",
      "na30_nb10_b32_linear_relu_d0 -> 0.8665532841761241\n",
      "na30_nb10_b64_linear_relu_d0 -> 0.8664185988231521\n",
      "na30_nb10_b64_relu_linear_d1 -> 0.8636069113628618\n",
      "na30_nb10_b32_relu_linear_d1 -> 0.862784712923163\n"
     ]
    }
   ],
   "source": [
    "results_2 = {}\n",
    "neurona_1 = 30\n",
    "for neurona_2 in [10, 30]:\n",
    "    for act_n1 in ['relu', 'linear']:\n",
    "        for act_n2 in ['relu', 'linear']:\n",
    "            for batch in [32, 64]:\n",
    "                for drop in [0, 1]:\n",
    "                    name = 'na{}_nb{}_b{}_{}_{}_d{}'.format(neurona_1, neurona_2, batch, act_n1, act_n2, drop)\n",
    "                    print('Testing ->', name)\n",
    "                    # Armo el modelo\n",
    "                    test_model = Sequential(name=name)\n",
    "\n",
    "                    test_model.add(Dense(neurona_1, activation=act_n1, input_shape=(x_temp.shape[1],)))\n",
    "                    if drop:\n",
    "                        test_model.add(Dropout(rate=0.2))\n",
    "                    test_model.add(Dense(neurona_2, activation=act_n2))\n",
    "\n",
    "                    test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "                    # Compilo el modelo\n",
    "                    test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "                    auc, metrics = train_model(x_temp, y_temp, test_model, name, batch, stopping_patiece=50)\n",
    "                    results_2[name] = (auc, metrics)\n",
    "                \n",
    "sorted_results_2 = list(reversed(sorted(results_2.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results_2[0:10]:\n",
    "    print(i[0], '->', i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "810969db-cbe4-435b-ba8d-437521ea3ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "na30_nb30_b32_linear_relu_d0_b -> 0.8699594066986605\n",
      "na30_nb30_b32_linear_relu_d1_b -> 0.8695607848555167\n",
      "na30_nb10_b64_relu_linear_d1_b -> 0.8626690444670577\n",
      "na30_nb10_b64_relu_linear_d0_b -> 0.8598345838677289\n"
     ]
    }
   ],
   "source": [
    "results_3 = {}\n",
    "redes = [[30, 10, 64, 'relu', 'linear'],\n",
    "         [30, 30, 32, 'linear', 'relu']]\n",
    "\n",
    "for n1, n2, b, a1, a2 in redes:\n",
    "    for drop in [0, 1]:\n",
    "        name = 'na{}_nb{}_b{}_{}_{}_d{}_b'.format(n1, n2, b, a1, a2, drop)\n",
    "        print('Testing ->', name)\n",
    "        # Armo el modelo\n",
    "        test_model = Sequential(name=name)\n",
    "\n",
    "        test_model.add(Dense(n1, activation=a1, input_shape=(x_temp.shape[1],)))\n",
    "        \n",
    "        test_model.add(Dense(n2, activation=a2))\n",
    "        \n",
    "        if drop:\n",
    "            test_model.add(Dropout(rate=0.5))\n",
    "        else:\n",
    "            test_model.add(BatchNormalization())\n",
    "            \n",
    "        test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "        # Compilo el modelo\n",
    "        test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "        auc, metrics = train_model(x_temp, y_temp, test_model, name, b, stopping_patiece=50)\n",
    "        results_3[name] = (auc, metrics)\n",
    "        clear_output(wait=True)\n",
    "                \n",
    "sorted_results_3 = list(reversed(sorted(results_3.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results_3:\n",
    "    print(i[0], '->', i[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256bac3-5304-454a-909f-1d8a7fd49676",
   "metadata": {},
   "source": [
    "Finalmente no se pudieron conseguir mejoras sustanciales agregando capas de Droput o BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825f34d-74bb-4b77-8f3f-2c5519e508cf",
   "metadata": {},
   "source": [
    "#### Optimizando hypperparametros del optimizador y regularizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcb469e3-9b5d-49e2-8634-73c8e245344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red1_lr0.05_l20.5_ -> 0.8721683525422185\n",
      "red1_lr0.02_l20.25_ -> 0.8715306728633037\n",
      "red1_lr0.05_l20.05_ -> 0.8713414803030963\n",
      "red1_lr0.01_l20.25_ -> 0.8700620315758952\n",
      "red1_lr0.02_l20.5_ -> 0.8693948471391499\n",
      "red1_lr0.05_l20.01_ -> 0.8684313149475351\n",
      "red1_lr0.05_l20.25_ -> 0.8674764384886956\n",
      "red1_lr0.01_l20.5_ -> 0.8669829990513234\n",
      "red1_lr0.05_l20.1_ -> 0.8664041795589345\n",
      "red1_lr0.02_l20.01_ -> 0.8658734829233747\n"
     ]
    }
   ],
   "source": [
    "results_4 = {}\n",
    "redes = [[30, 10, 64, 'relu', 'linear'],\n",
    "         [30, 30, 32, 'linear', 'relu']]\n",
    "\n",
    "for i in [0, 1]:\n",
    "    n1, n2, b, a1, a2 = redes[i]\n",
    "    for lr in [5e-3, 0.01, 0.02, 0.05, 0.1, 0.5]:\n",
    "        for l_2 in [0.01, 0.05, 0.1, 0.25, 0.5]:\n",
    "            name = 'red{}_lr{}_l2{}_'.format(i, lr, l_2)\n",
    "            print('Testing ->', name)\n",
    "            # Armo el modelo\n",
    "            test_model = Sequential(name=name)\n",
    "\n",
    "            test_model.add(Dense(n1, activation=a1, input_shape=(x_temp.shape[1],)))\n",
    "\n",
    "            test_model.add(Dense(n2, activation=a2))\n",
    "\n",
    "            test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(l_2)))\n",
    "            # Compilo el modelo\n",
    "            test_model.compile(optimizer=Adam(learning_rate=lr), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "            auc, metrics = train_model(x_temp, y_temp, test_model, name, b, stopping_patiece=50)\n",
    "            results_4[name] = (auc, metrics)\n",
    "            clear_output(wait=True)\n",
    "                \n",
    "sorted_results_4 = list(reversed(sorted(results_4.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results_4[:10]:\n",
    "    print(i[0], '->', i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55624b90-ad70-42f6-9940-540b83ca4ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
