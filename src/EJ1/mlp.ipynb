{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eadad356-bc8b-4ae3-af46-51624e9b6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from os.path import join\n",
    "from os import getcwd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a0664c46-861a-429d-a736-14b3d54276ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, auc\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "87e9ea86-22bd-4c88-8937-c34f49ce6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import AUC # Area under the curve, default: ROC\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.initializers import GlorotNormal\n",
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fec708f-306d-4a12-9d9f-7f8020bb3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4822c966-b64f-4ca0-9604-c9cd4b875624",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = getcwd()+'\\\\checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a024e19-bfc5-45b0-b492-e57323f6183f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../databases/diabetes.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "708d29ba-8b33-4740-909e-654070c43da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlayers = {\n",
    "    'BloodPressure': (40, np.Inf),\n",
    "    'SkinThickness': (0, 80),\n",
    "    'Insulin': (0, 400),\n",
    "    'BMI': (0, 50)\n",
    "}\n",
    "\n",
    "zeros = [\n",
    "    'Glucose',\n",
    "    'BloodPressure',\n",
    "    'SkinThickness',\n",
    "    'Insulin',\n",
    "    'BMI'\n",
    "]\n",
    "x_df = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
    "y_df = df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "077675fa-81fe-4dad-b1a2-af4a9384cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_changing_state(x_dataset, y_dataset, model, state, direction, my_callbacks):\n",
    "    # Split dataset into 15% test, 85% train \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_df, y_df, test_size=0.15, random_state=state)\n",
    "    # Reemplazo los valores nulos y los outlayer\n",
    "    x_train_clean, _data = replace_outliers_zeros(x_train, outlayers, zeros, mean_median=True)\n",
    "    x_valid_clean, _data = replace_outliers_zeros(x_valid, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "    # Normalizo los datasets\n",
    "    x_train_norm, _norm_dict = normalize(x_train_clean, None)\n",
    "    x_valid_norm, _norm_dict = normalize(x_valid_clean, _norm_dict)\n",
    "    # Train model\n",
    "    history_mlp_0 = mlp_model.fit(x_train_norm, y_train, validation_data=(x_valid_norm, y_valid),\n",
    "                              batch_size=32, epochs=200,\n",
    "                              verbose=0, callbacks=my_callbacks) \n",
    "    # Cargo el mejor modelo entrenado\n",
    "    mlp_model.load_weights(direction)\n",
    "    results = verify_model(mlp_model, x_train_norm, y_train, x_valid_norm, y_valid)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1bab0fcf-e5f0-4578-a471-b4de4003fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into 15% test, 85% train \n",
    "x_temp, x_test, y_temp, y_test = train_test_split(x_df, y_df, test_size=0.15)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_temp, y_temp, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e1928f7a-ec8c-45e1-b11f-ff653b4ab78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazo los valores nulos y los outlayer\n",
    "x_train_clean, _data = replace_outliers_zeros(x_train, outlayers, zeros, mean_median=True)\n",
    "x_test_clean, _data = replace_outliers_zeros(x_test, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "x_valid_clean, _data = replace_outliers_zeros(x_valid, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "# Normalizo los datasets\n",
    "x_train_norm, _norm_dict = normalize(x_train_clean, None)\n",
    "x_valid_norm, _norm_dict = normalize(x_valid_clean, _norm_dict)\n",
    "x_test_norm, _norm_dict = normalize(x_test_clean, _norm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ffc8e915-c772-4560-a6f5-434ecee7a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = EarlyStopping(monitor='val_auc', patience=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ce718-b28b-4d57-a0d7-3a99e8d02098",
   "metadata": {},
   "source": [
    "### Definición de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0135178c-9c0a-44c1-b1dc-4a3bfbd2f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlp_0\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_0_checkpoint_callback = ModelCheckpoint(filepath=join(checkpoints_path, 'mlp_0'), save_weights_only=True, monitor='val_auc', mode='max', \n",
    "                                           save_best_only=True)\n",
    "mlp_model = Sequential(name='mlp_0')\n",
    "\n",
    "mlp_model.add(Dense(10, activation='relu', input_shape=(x_train_norm.shape[1],)))\n",
    "mlp_model.add(Dense(5, activation='linear'))\n",
    "mlp_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5efacb4-e902-4b98-8aca-42089e2a1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c4e0371-ae2c-4392-9e57-c3ec22564c62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 1s 58ms/step - loss: 0.7705 - auc: 0.7599 - val_loss: 0.6159 - val_auc: 0.7702\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 0.5413 - auc: 0.8417 - val_loss: 0.5418 - val_auc: 0.7830\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4856 - auc: 0.8529 - val_loss: 0.5331 - val_auc: 0.7924\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4670 - auc: 0.8573 - val_loss: 0.5731 - val_auc: 0.7780\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4805 - auc: 0.8454 - val_loss: 0.5519 - val_auc: 0.7820\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4615 - auc: 0.8608 - val_loss: 0.5442 - val_auc: 0.7833\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4559 - auc: 0.8640 - val_loss: 0.5429 - val_auc: 0.7778\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4484 - auc: 0.8662 - val_loss: 0.5631 - val_auc: 0.7720\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4504 - auc: 0.8656 - val_loss: 0.5472 - val_auc: 0.7801\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4436 - auc: 0.8714 - val_loss: 0.6425 - val_auc: 0.7741\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4509 - auc: 0.8644 - val_loss: 0.5414 - val_auc: 0.7731\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4357 - auc: 0.8727 - val_loss: 0.5888 - val_auc: 0.7694\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4360 - auc: 0.8751 - val_loss: 0.5520 - val_auc: 0.7702\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4255 - auc: 0.8813 - val_loss: 0.5745 - val_auc: 0.7715\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4270 - auc: 0.8789 - val_loss: 0.5827 - val_auc: 0.7692\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4152 - auc: 0.8869 - val_loss: 0.5804 - val_auc: 0.7700\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4146 - auc: 0.8865 - val_loss: 0.5851 - val_auc: 0.7686\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4168 - auc: 0.8850 - val_loss: 0.6005 - val_auc: 0.7694\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4124 - auc: 0.8877 - val_loss: 0.6015 - val_auc: 0.7608\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4096 - auc: 0.8884 - val_loss: 0.6562 - val_auc: 0.7507\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4095 - auc: 0.8890 - val_loss: 0.6014 - val_auc: 0.7530\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4122 - auc: 0.8886 - val_loss: 0.6418 - val_auc: 0.7577\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4141 - auc: 0.8882 - val_loss: 0.6616 - val_auc: 0.7556\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4097 - auc: 0.8913 - val_loss: 0.6477 - val_auc: 0.7410\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4032 - auc: 0.8936 - val_loss: 0.6633 - val_auc: 0.7467\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4060 - auc: 0.8919 - val_loss: 0.6625 - val_auc: 0.7501\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3964 - auc: 0.8971 - val_loss: 0.6306 - val_auc: 0.7556\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3982 - auc: 0.8957 - val_loss: 0.7006 - val_auc: 0.7543\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3938 - auc: 0.8977 - val_loss: 0.6408 - val_auc: 0.7517\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3916 - auc: 0.8992 - val_loss: 0.6388 - val_auc: 0.7491\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3904 - auc: 0.8989 - val_loss: 0.6522 - val_auc: 0.7504\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3880 - auc: 0.9008 - val_loss: 0.6839 - val_auc: 0.7533\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3890 - auc: 0.9004 - val_loss: 0.6971 - val_auc: 0.7522\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3850 - auc: 0.9027 - val_loss: 0.6403 - val_auc: 0.7504\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3875 - auc: 0.9010 - val_loss: 0.6776 - val_auc: 0.7535\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3819 - auc: 0.9035 - val_loss: 0.6855 - val_auc: 0.7538\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3835 - auc: 0.9040 - val_loss: 0.7131 - val_auc: 0.7470\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3873 - auc: 0.9026 - val_loss: 0.6358 - val_auc: 0.7514\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3888 - auc: 0.9009 - val_loss: 0.7560 - val_auc: 0.7433\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3841 - auc: 0.9029 - val_loss: 0.6806 - val_auc: 0.7441\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3787 - auc: 0.9058 - val_loss: 0.7166 - val_auc: 0.7470\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3766 - auc: 0.9066 - val_loss: 0.6704 - val_auc: 0.7496\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3777 - auc: 0.9057 - val_loss: 0.6882 - val_auc: 0.7462\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3760 - auc: 0.9061 - val_loss: 0.6687 - val_auc: 0.7559\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3890 - auc: 0.9019 - val_loss: 0.6273 - val_auc: 0.7470\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3906 - auc: 0.8995 - val_loss: 0.7813 - val_auc: 0.7483\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3789 - auc: 0.9052 - val_loss: 0.7343 - val_auc: 0.7457\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3751 - auc: 0.9069 - val_loss: 0.6811 - val_auc: 0.7460\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3759 - auc: 0.9069 - val_loss: 0.7002 - val_auc: 0.7527\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3772 - auc: 0.9054 - val_loss: 0.6537 - val_auc: 0.7439\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3742 - auc: 0.9076 - val_loss: 0.6924 - val_auc: 0.7504\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3794 - auc: 0.9062 - val_loss: 0.6381 - val_auc: 0.7514\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3772 - auc: 0.9075 - val_loss: 0.7664 - val_auc: 0.7441\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3791 - auc: 0.9045 - val_loss: 0.6804 - val_auc: 0.7426\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3782 - auc: 0.9065 - val_loss: 0.7257 - val_auc: 0.7475\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3739 - auc: 0.9088 - val_loss: 0.6877 - val_auc: 0.7444\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3709 - auc: 0.9100 - val_loss: 0.7114 - val_auc: 0.7491\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3695 - auc: 0.9103 - val_loss: 0.7014 - val_auc: 0.7548\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3730 - auc: 0.9093 - val_loss: 0.7397 - val_auc: 0.7439\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3705 - auc: 0.9104 - val_loss: 0.6834 - val_auc: 0.7415\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3687 - auc: 0.9122 - val_loss: 0.7553 - val_auc: 0.7512\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3784 - auc: 0.9058 - val_loss: 0.8391 - val_auc: 0.7457\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3722 - auc: 0.9091 - val_loss: 0.7119 - val_auc: 0.7470\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3709 - auc: 0.9095 - val_loss: 0.6968 - val_auc: 0.7389\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3672 - auc: 0.9118 - val_loss: 0.7638 - val_auc: 0.7431\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3666 - auc: 0.9111 - val_loss: 0.7006 - val_auc: 0.7535\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3655 - auc: 0.9127 - val_loss: 0.7162 - val_auc: 0.7405\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3619 - auc: 0.9142 - val_loss: 0.7195 - val_auc: 0.7507\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3663 - auc: 0.9118 - val_loss: 0.7785 - val_auc: 0.7517\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3775 - auc: 0.9052 - val_loss: 0.7228 - val_auc: 0.7480\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3797 - auc: 0.9084 - val_loss: 0.6225 - val_auc: 0.7493\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3749 - auc: 0.9100 - val_loss: 0.7376 - val_auc: 0.7499\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3690 - auc: 0.9099 - val_loss: 0.7889 - val_auc: 0.7360\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3694 - auc: 0.9089 - val_loss: 0.7282 - val_auc: 0.7467\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3672 - auc: 0.9120 - val_loss: 0.7366 - val_auc: 0.7470\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3628 - auc: 0.9136 - val_loss: 0.7091 - val_auc: 0.7376\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3655 - auc: 0.9127 - val_loss: 0.7621 - val_auc: 0.7444\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3630 - auc: 0.9126 - val_loss: 0.7038 - val_auc: 0.7394\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3589 - auc: 0.9158 - val_loss: 0.7778 - val_auc: 0.7400\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3599 - auc: 0.9143 - val_loss: 0.7597 - val_auc: 0.7478\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3659 - auc: 0.9127 - val_loss: 0.7223 - val_auc: 0.7470\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3642 - auc: 0.9129 - val_loss: 0.7598 - val_auc: 0.7439\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3601 - auc: 0.9152 - val_loss: 0.7719 - val_auc: 0.7452\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3561 - auc: 0.9173 - val_loss: 0.7531 - val_auc: 0.7384\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3579 - auc: 0.9175 - val_loss: 0.6364 - val_auc: 0.7444\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3600 - auc: 0.9152 - val_loss: 0.7697 - val_auc: 0.7321\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3582 - auc: 0.9154 - val_loss: 0.7015 - val_auc: 0.7415\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3592 - auc: 0.9164 - val_loss: 0.7113 - val_auc: 0.7350\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3554 - auc: 0.9168 - val_loss: 0.8402 - val_auc: 0.7394\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3553 - auc: 0.9169 - val_loss: 0.7265 - val_auc: 0.7389\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3498 - auc: 0.9197 - val_loss: 0.7549 - val_auc: 0.7381\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3585 - auc: 0.9159 - val_loss: 0.7234 - val_auc: 0.7436\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3532 - auc: 0.9193 - val_loss: 0.7148 - val_auc: 0.7368\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3762 - auc: 0.9087 - val_loss: 0.9565 - val_auc: 0.7337\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3578 - auc: 0.9172 - val_loss: 0.7505 - val_auc: 0.7389\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3534 - auc: 0.9190 - val_loss: 0.7452 - val_auc: 0.7337\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3522 - auc: 0.9196 - val_loss: 0.7900 - val_auc: 0.7329\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3570 - auc: 0.9168 - val_loss: 0.7031 - val_auc: 0.7366\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3497 - auc: 0.9201 - val_loss: 0.8083 - val_auc: 0.7439\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3515 - auc: 0.9199 - val_loss: 0.8151 - val_auc: 0.7387\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3533 - auc: 0.9194 - val_loss: 0.6566 - val_auc: 0.7360\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3601 - auc: 0.9172 - val_loss: 0.7591 - val_auc: 0.7379\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3469 - auc: 0.9209 - val_loss: 0.8310 - val_auc: 0.7426\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3517 - auc: 0.9200 - val_loss: 0.6716 - val_auc: 0.7347\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3563 - auc: 0.9182 - val_loss: 0.8622 - val_auc: 0.7392\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3603 - auc: 0.9154 - val_loss: 0.7582 - val_auc: 0.7418\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3615 - auc: 0.9147 - val_loss: 0.7883 - val_auc: 0.7405\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3529 - auc: 0.9215 - val_loss: 0.7752 - val_auc: 0.7324\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3500 - auc: 0.9196 - val_loss: 0.8929 - val_auc: 0.7269\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3509 - auc: 0.9236 - val_loss: 0.7437 - val_auc: 0.7436\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3561 - auc: 0.9192 - val_loss: 0.7208 - val_auc: 0.7368\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3474 - auc: 0.9213 - val_loss: 0.8295 - val_auc: 0.7298\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3475 - auc: 0.9220 - val_loss: 0.8463 - val_auc: 0.7366\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3504 - auc: 0.9215 - val_loss: 0.8037 - val_auc: 0.7230\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3505 - auc: 0.9196 - val_loss: 0.7657 - val_auc: 0.7267\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3477 - auc: 0.9219 - val_loss: 0.7613 - val_auc: 0.7225\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3458 - auc: 0.9229 - val_loss: 0.8675 - val_auc: 0.7379\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3495 - auc: 0.9227 - val_loss: 0.6679 - val_auc: 0.7170\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3543 - auc: 0.9212 - val_loss: 0.8390 - val_auc: 0.7238\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3408 - auc: 0.9243 - val_loss: 0.7316 - val_auc: 0.7209\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3449 - auc: 0.9231 - val_loss: 0.8610 - val_auc: 0.7199\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3423 - auc: 0.9233 - val_loss: 0.7569 - val_auc: 0.7290\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3415 - auc: 0.9246 - val_loss: 0.7450 - val_auc: 0.7214\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3443 - auc: 0.9234 - val_loss: 0.7395 - val_auc: 0.7243\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3592 - auc: 0.9205 - val_loss: 0.7286 - val_auc: 0.7319\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3379 - auc: 0.9254 - val_loss: 0.8778 - val_auc: 0.7178\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3405 - auc: 0.9239 - val_loss: 0.8310 - val_auc: 0.7126\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3430 - auc: 0.9234 - val_loss: 0.8785 - val_auc: 0.7165\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3413 - auc: 0.9245 - val_loss: 0.7345 - val_auc: 0.7220\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3447 - auc: 0.9227 - val_loss: 0.8317 - val_auc: 0.7212\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3360 - auc: 0.9268 - val_loss: 0.8326 - val_auc: 0.7254\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3442 - auc: 0.9241 - val_loss: 0.7482 - val_auc: 0.7094\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3411 - auc: 0.9263 - val_loss: 0.8379 - val_auc: 0.7173\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3436 - auc: 0.9244 - val_loss: 0.9083 - val_auc: 0.7188\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3447 - auc: 0.9231 - val_loss: 0.8084 - val_auc: 0.7212\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3364 - auc: 0.9273 - val_loss: 1.0142 - val_auc: 0.7152\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3456 - auc: 0.9250 - val_loss: 0.7325 - val_auc: 0.7152\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3403 - auc: 0.9259 - val_loss: 0.8965 - val_auc: 0.7186\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3342 - auc: 0.9275 - val_loss: 0.7785 - val_auc: 0.7196\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3413 - auc: 0.9242 - val_loss: 0.8806 - val_auc: 0.7105\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3374 - auc: 0.9258 - val_loss: 0.7934 - val_auc: 0.7154\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3383 - auc: 0.9263 - val_loss: 0.8893 - val_auc: 0.7123\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3449 - auc: 0.9244 - val_loss: 0.6954 - val_auc: 0.7214\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3416 - auc: 0.9254 - val_loss: 0.8654 - val_auc: 0.7136\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3349 - auc: 0.9280 - val_loss: 0.7601 - val_auc: 0.7178\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3390 - auc: 0.9253 - val_loss: 0.8347 - val_auc: 0.7217\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3394 - auc: 0.9242 - val_loss: 0.7025 - val_auc: 0.7180\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3347 - auc: 0.9277 - val_loss: 0.8518 - val_auc: 0.7136\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3378 - auc: 0.9250 - val_loss: 0.8684 - val_auc: 0.7042\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3380 - auc: 0.9258 - val_loss: 0.8300 - val_auc: 0.7128\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3330 - auc: 0.9282 - val_loss: 0.8697 - val_auc: 0.7141\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3369 - auc: 0.9257 - val_loss: 0.7641 - val_auc: 0.7160\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3369 - auc: 0.9271 - val_loss: 0.7382 - val_auc: 0.7162\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3429 - auc: 0.9237 - val_loss: 0.7578 - val_auc: 0.7076\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3342 - auc: 0.9294 - val_loss: 0.9310 - val_auc: 0.7144\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3363 - auc: 0.9266 - val_loss: 0.8030 - val_auc: 0.7178\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3324 - auc: 0.9288 - val_loss: 0.8649 - val_auc: 0.7063\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3402 - auc: 0.9246 - val_loss: 0.7554 - val_auc: 0.7134\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3335 - auc: 0.9282 - val_loss: 0.9531 - val_auc: 0.7126\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3325 - auc: 0.9280 - val_loss: 0.8006 - val_auc: 0.7204\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3303 - auc: 0.9282 - val_loss: 0.8142 - val_auc: 0.7186\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3310 - auc: 0.9293 - val_loss: 0.8428 - val_auc: 0.7160\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3359 - auc: 0.9261 - val_loss: 0.8358 - val_auc: 0.7173\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3309 - auc: 0.9286 - val_loss: 0.7765 - val_auc: 0.7154\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3365 - auc: 0.9261 - val_loss: 0.8143 - val_auc: 0.7188\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3368 - auc: 0.9290 - val_loss: 0.7900 - val_auc: 0.7063\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3322 - auc: 0.9287 - val_loss: 0.8646 - val_auc: 0.7212\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3297 - auc: 0.9302 - val_loss: 0.7421 - val_auc: 0.7131\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3320 - auc: 0.9281 - val_loss: 0.8162 - val_auc: 0.7136\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3365 - auc: 0.9273 - val_loss: 0.8501 - val_auc: 0.7008\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3328 - auc: 0.9296 - val_loss: 0.7402 - val_auc: 0.7238\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3270 - auc: 0.9309 - val_loss: 0.8364 - val_auc: 0.7126\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3312 - auc: 0.9284 - val_loss: 0.8093 - val_auc: 0.7087\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3314 - auc: 0.9292 - val_loss: 0.7751 - val_auc: 0.7126\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4013 - auc: 0.887 - 0s 2ms/step - loss: 0.3360 - auc: 0.9267 - val_loss: 0.7670 - val_auc: 0.7089\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3356 - auc: 0.9295 - val_loss: 0.9948 - val_auc: 0.7209\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3363 - auc: 0.9276 - val_loss: 0.8147 - val_auc: 0.7154\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3281 - auc: 0.9310 - val_loss: 0.9181 - val_auc: 0.7053\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3391 - auc: 0.9254 - val_loss: 0.8472 - val_auc: 0.7152\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3318 - auc: 0.9293 - val_loss: 0.8963 - val_auc: 0.7186\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3301 - auc: 0.9300 - val_loss: 0.6742 - val_auc: 0.7160\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3427 - auc: 0.9253 - val_loss: 0.9377 - val_auc: 0.7134\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3306 - auc: 0.9303 - val_loss: 0.8860 - val_auc: 0.7102\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3397 - auc: 0.9267 - val_loss: 0.9322 - val_auc: 0.7121\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3397 - auc: 0.9284 - val_loss: 0.6750 - val_auc: 0.7076\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3386 - auc: 0.9282 - val_loss: 0.9609 - val_auc: 0.7076\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3354 - auc: 0.9275 - val_loss: 0.9134 - val_auc: 0.7134\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3312 - auc: 0.9299 - val_loss: 0.7750 - val_auc: 0.7084\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3346 - auc: 0.9282 - val_loss: 0.7477 - val_auc: 0.7107\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3305 - auc: 0.9295 - val_loss: 0.8309 - val_auc: 0.7032\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3288 - auc: 0.9309 - val_loss: 0.8472 - val_auc: 0.7016\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3298 - auc: 0.9298 - val_loss: 0.8843 - val_auc: 0.7269\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3272 - auc: 0.9306 - val_loss: 0.7281 - val_auc: 0.7008\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3313 - auc: 0.9311 - val_loss: 0.9497 - val_auc: 0.7136\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3269 - auc: 0.9318 - val_loss: 0.9596 - val_auc: 0.7066\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3361 - auc: 0.9283 - val_loss: 0.7240 - val_auc: 0.7141\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3247 - auc: 0.9323 - val_loss: 0.9057 - val_auc: 0.7123\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3263 - auc: 0.9321 - val_loss: 0.8496 - val_auc: 0.7084\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3247 - auc: 0.9315 - val_loss: 0.9094 - val_auc: 0.7123\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3257 - auc: 0.9315 - val_loss: 0.7481 - val_auc: 0.7162\n"
     ]
    }
   ],
   "source": [
    "history_mlp_0 = mlp_model.fit(x_train_norm, y_train, validation_data=(x_valid_norm, y_valid),\n",
    "                              batch_size=32, epochs=200,\n",
    "                              verbose=1, callbacks=[mlp_0_checkpoint_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b2468623-f729-4536-b0aa-0376dec4f190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set</th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Especificidad</th>\n",
       "      <th>Sensibilidad</th>\n",
       "      <th>Valor Predictivo Positivo</th>\n",
       "      <th>Valor Predictivo Negativo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>0.858927</td>\n",
       "      <td>0.887006</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.821990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Validacion</td>\n",
       "      <td>0.792906</td>\n",
       "      <td>0.760563</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.830769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Set   AUC ROC  Especificidad  Sensibilidad  \\\n",
       "0       Train  0.858927       0.887006      0.660000   \n",
       "1  Validacion  0.792906       0.760563      0.592593   \n",
       "\n",
       "   Valor Predictivo Positivo  Valor Predictivo Negativo  \n",
       "0                   0.767442                   0.821990  \n",
       "1                   0.484848                   0.830769  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargo el mejor modelo entrenado\n",
    "mlp_model.load_weights(join(checkpoints_path, 'mlp_0'))\n",
    "verify_model(mlp_model, x_train_norm, y_train, x_valid_norm, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d906e-cc88-413d-a538-acf90b7fc195",
   "metadata": {},
   "source": [
    "Salieron resultados interesantes, vamos a verificar si se mantiene al cambiar el random state al hacer split de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e491747-9265-4e72-9cbe-de10788cbfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlp_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_state = Sequential(name='mlp_1')\n",
    "mlp_state.add(Dense(10, activation='relu', input_shape=(x_train_norm.shape[1],)))\n",
    "mlp_state.add(Dense(5, activation='linear'))\n",
    "mlp_state.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "mlp_state.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "pesos_inicial = mlp_state.get_weights()\n",
    "mlp_state.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c79987f8-7c3f-4504-9e03-1bfed5c67d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing state 0\n",
      "Runing state 1\n",
      "Runing state 2\n",
      "Runing state 3\n",
      "Runing state 4\n",
      "Runing state 5\n",
      "Runing state 6\n",
      "Runing state 7\n",
      "Runing state 8\n",
      "Runing state 9\n"
     ]
    }
   ],
   "source": [
    "train_auc = []\n",
    "valid_auc = []\n",
    "results = []\n",
    "\n",
    "for i in range(10):\n",
    "    print('Runing state', i)\n",
    "    name  = 'mlp_state_{}'.format(i)\n",
    "    mlp_state.set_weights(pesos_inicial)\n",
    "    checkdir = join(checkpoints_path, name)\n",
    "    temp_callback = ModelCheckpoint(filepath=checkdir, save_weights_only=True, monitor='val_auc', mode='max', save_best_only=True)\n",
    "    metrics = run_changing_state(x_temp, y_temp, mlp_state, i, checkdir, [temp_callback])\n",
    "    results.append(metrics)\n",
    "    train_auc.append(metrics['AUC ROC'][0])\n",
    "    valid_auc.append(metrics['AUC ROC'][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9cdace0c-54bb-411b-93c1-4c1111d267d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_auc, label='Train')\n",
    "plt.plot(valid_auc, label='Validation')\n",
    "mean = np.mean(valid_auc)\n",
    "std = np.std(valid_auc)\n",
    "plt.hlines([mean+std, mean, mean-std], 0, 9)   \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc3079-2d04-4d44-b345-c8a8be95bf84",
   "metadata": {},
   "source": [
    "Se puede observar que en la mayoria de los casos la metrica de validación da mejor que la metrica de train, con lo cual se puede inferir que el modelo tendrá una gran varianza debido a la poca cantidad de datos disponible para los parámetros que requiere entrenar el modelo. Por lo tanto una mejor opcion es utilizar k-folding en lugar de hold out validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901419e-6962-4b1b-9ed4-a1a2acfa0e54",
   "metadata": {},
   "source": [
    "Ahora vamos a probar utilizar 1 o 2 capas ocultas, variando la cantidad de neuronas entre 5, 10 y 30, modificando la activacion entre relu y lineal. Para ello utilizaremos la tecnica de k-folding y la metrica de cada set de hyperparámetros sera el promedio de las metricas de cada fold. Una vez testeado esto se continuara buscando mejorar el modelo cambiando la regularizacion o agregando capas de Droput o Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "32deaee6-b847-4210-9f3d-944edec57fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folding = KFold(n_splits=5)\n",
    "\n",
    "def train_model(x_dataset, y_dataset, model, name, batch, stopping_patiece=None):\n",
    "    pesos_default = model.get_weights()\n",
    "    folds = folding.split(x_dataset)\n",
    "    auc_val = []\n",
    "    results = []\n",
    "    curr = 0\n",
    "    for train, valid in folds:\n",
    "        # Split dataset\n",
    "        x_train, x_valid = x_dataset.iloc[train], x_dataset.iloc[valid]\n",
    "        y_train, y_valid = y_dataset.iloc[train], y_dataset.iloc[valid]\n",
    "        \n",
    "        # Proceso los datos\n",
    "        x_train_c, _data = replace_outliers_zeros(x_train, outlayers, zeros, mean_median=True)\n",
    "        x_valid_c, _data = replace_outliers_zeros(x_valid, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "        # Normalizo los datasets\n",
    "        x_train_n, _norm_dict = normalize(x_train_c, None)\n",
    "        x_valid_n, _norm_dict = normalize(x_valid_c, _norm_dict)\n",
    "    \n",
    "        # Create callbacks\n",
    "        checkdir = join(checkpoints_path, name+'f{}'.format(curr))\n",
    "        temp_callback = ModelCheckpoint(filepath=checkdir, save_weights_only=True, monitor='val_auc', mode='max', save_best_only=True)\n",
    "        my_callbacks = [temp_callback]\n",
    "        if stopping_patiece:\n",
    "            my_callbacks.append(EarlyStopping(monitor='val_auc', patience=stopping_patiece))\n",
    "        # Train model\n",
    "        history = model.fit(x_train_n, y_train, validation_data=(x_valid_n, y_valid),\n",
    "                            batch_size=batch, epochs=200,\n",
    "                            verbose=0, callbacks=my_callbacks) \n",
    "        # Cargo el mejor modelo entrenado\n",
    "        model.load_weights(checkdir)\n",
    "        metrics = verify_model(model, x_train_n, y_train, x_valid_n, y_valid)\n",
    "        auc_val.append(metrics['AUC ROC'][1])\n",
    "        results.append(metrics)\n",
    "        # Reset model for next training\n",
    "        model.set_weights(pesos_default)\n",
    "    return np.mean(auc_val), results    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2080a7-310c-4822-8a1f-ed6c835c5dc3",
   "metadata": {},
   "source": [
    "1 capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "633e8759-b00f-4a5b-88c2-7606855946a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing -> n5_b32_linear\n",
      "Auc score =  0.8465187586075779\n",
      "Testing -> n5_b64_linear\n",
      "Auc score =  0.8435372247737304\n",
      "Testing -> n5_b32_relu\n",
      "Auc score =  0.8512925704116805\n",
      "Testing -> n5_b64_relu\n",
      "Auc score =  0.8348620106520915\n",
      "Testing -> n10_b32_linear\n",
      "Auc score =  0.8442330867064156\n",
      "Testing -> n10_b64_linear\n",
      "Auc score =  0.8419770482188458\n",
      "Testing -> n10_b32_relu\n",
      "Auc score =  0.8519265572777279\n",
      "Testing -> n10_b64_relu\n",
      "Auc score =  0.8515506365062905\n",
      "Testing -> n30_b32_linear\n",
      "Auc score =  0.8443438546762918\n",
      "Testing -> n30_b64_linear\n",
      "Auc score =  0.8454345812591655\n",
      "Testing -> n30_b32_relu\n",
      "Auc score =  0.855564919677362\n",
      "Testing -> n30_b64_relu\n",
      "Auc score =  0.85330703757104\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for neuronas in [5, 10, 30]:\n",
    "    for activation in ['linear', 'relu']:\n",
    "        for batch in [32, 64]:\n",
    "            name = 'n{}_b{}_{}'.format(neuronas, batch, activation)\n",
    "            print('Testing ->', name)\n",
    "            # Armo el modelo\n",
    "            test_model = Sequential(name=name)\n",
    "            test_model.add(Dense(neuronas, activation=activation, input_shape=(x_temp.shape[1],)))\n",
    "            test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "            # Compilo el modelo\n",
    "            test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "            auc, metrics = train_model(x_temp, y_temp, test_model, name, batch, stopping_patiece=50)\n",
    "            results[name] = (auc, metrics)\n",
    "            print('Auc score = ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b522c119-5a74-4aaf-b315-63453017dd68",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing -> na5_nb5_b32_linear\n",
      "Testing -> na5_nb5_b64_linear\n",
      "Testing -> na5_nb5_b32_relu\n",
      "Testing -> na5_nb5_b64_relu\n",
      "Testing -> na5_nb10_b32_linear\n",
      "Testing -> na5_nb10_b64_linear\n",
      "Testing -> na5_nb10_b32_relu\n",
      "Testing -> na5_nb10_b64_relu\n",
      "Testing -> na5_nb30_b32_linear\n",
      "Testing -> na5_nb30_b64_linear\n",
      "Testing -> na5_nb30_b32_relu\n",
      "Testing -> na5_nb30_b64_relu\n",
      "Testing -> na10_nb5_b32_linear\n",
      "Testing -> na10_nb5_b64_linear\n",
      "Testing -> na10_nb5_b32_relu\n",
      "Testing -> na10_nb5_b64_relu\n",
      "Testing -> na10_nb10_b32_linear\n",
      "Testing -> na10_nb10_b64_linear\n",
      "Testing -> na10_nb10_b32_relu\n",
      "Testing -> na10_nb10_b64_relu\n",
      "Testing -> na10_nb30_b32_linear\n",
      "Testing -> na10_nb30_b64_linear\n",
      "Testing -> na10_nb30_b32_relu\n",
      "Testing -> na10_nb30_b64_relu\n",
      "Testing -> na30_nb5_b32_linear\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0156s). Check your callbacks.\n",
      "Testing -> na30_nb5_b64_linear\n",
      "Testing -> na30_nb5_b32_relu\n",
      "Testing -> na30_nb5_b64_relu\n",
      "Testing -> na30_nb10_b32_linear\n",
      "Testing -> na30_nb10_b64_linear\n",
      "Testing -> na30_nb10_b32_relu\n",
      "Testing -> na30_nb10_b64_relu\n",
      "Testing -> na30_nb30_b32_linear\n",
      "Testing -> na30_nb30_b64_linear\n",
      "Testing -> na30_nb30_b32_relu\n",
      "Testing -> na30_nb30_b64_relu\n"
     ]
    }
   ],
   "source": [
    "for neuronas_1 in [5, 10, 30]:\n",
    "    for neuronas_2 in [5, 10, 30]:\n",
    "        for activation in ['linear', 'relu']:\n",
    "            for batch in [32, 64]:\n",
    "                name = 'na{}_nb{}_b{}_{}'.format(neuronas_1, neuronas_2, batch, activation)\n",
    "                print('Testing ->', name)\n",
    "                # Armo el modelo\n",
    "                test_model = Sequential(name=name)\n",
    "                test_model.add(Dense(neuronas_1, activation=activation, input_shape=(x_temp.shape[1],)))\n",
    "                test_model.add(Dense(neuronas_2, activation=activation))\n",
    "                test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "                # Compilo el modelo\n",
    "                test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "                auc, metrics = train_model(x_temp, y_temp, test_model, name, batch, stopping_patiece=50)\n",
    "                results[name] = (auc, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe383c-2854-4ae0-8838-738747eedd59",
   "metadata": {},
   "source": [
    "Veo los 10 mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "91b488f9-fcef-4761-b17c-a38956166f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "na30_nb30_b64_relu -> 0.861352840215399\n",
      "na30_nb10_b32_relu -> 0.8582115959904792\n",
      "na30_nb10_b64_relu -> 0.8566529766960528\n",
      "n30_b32_relu -> 0.855564919677362\n",
      "na30_nb30_b32_relu -> 0.8551248583486846\n",
      "na30_nb5_b64_relu -> 0.8539013062773699\n",
      "n30_b64_relu -> 0.85330703757104\n",
      "na10_nb30_b32_relu -> 0.8531993851794029\n",
      "na10_nb5_b32_relu -> 0.8529004140800847\n",
      "na10_nb10_b32_relu -> 0.8521677479696826\n"
     ]
    }
   ],
   "source": [
    "sorted_results = list(reversed(sorted(results.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results[0:10]:\n",
    "    print(i[0], '->', i[1][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c6fbf-5ebd-4482-84d8-8760d599e0e5",
   "metadata": {},
   "source": [
    "Se puede observar como los de mejor performance son en su mayoria redes con 2 capas ocultas, y con gran cantidad de neuronas en cada una. Con estos resultados se buscara optimizar los hyperparámetros de los 3 mejores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "26a31e61-2ea6-4415-807f-312dd67f09ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing -> na30_nb10_b32_relu_relu_d0\n",
      "Testing -> na30_nb10_b32_relu_relu_d1\n",
      "Testing -> na30_nb10_b64_relu_relu_d0\n",
      "Testing -> na30_nb10_b64_relu_relu_d1\n",
      "Testing -> na30_nb10_b32_relu_linear_d0\n",
      "Testing -> na30_nb10_b32_relu_linear_d1\n",
      "Testing -> na30_nb10_b64_relu_linear_d0\n",
      "Testing -> na30_nb10_b64_relu_linear_d1\n",
      "Testing -> na30_nb10_b32_linear_relu_d0\n",
      "Testing -> na30_nb10_b32_linear_relu_d1\n",
      "Testing -> na30_nb10_b64_linear_relu_d0\n",
      "Testing -> na30_nb10_b64_linear_relu_d1\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0156s). Check your callbacks.\n",
      "Testing -> na30_nb10_b32_linear_linear_d0\n",
      "Testing -> na30_nb10_b32_linear_linear_d1\n",
      "Testing -> na30_nb10_b64_linear_linear_d0\n",
      "Testing -> na30_nb10_b64_linear_linear_d1\n",
      "Testing -> na30_nb30_b32_relu_relu_d0\n",
      "Testing -> na30_nb30_b32_relu_relu_d1\n",
      "Testing -> na30_nb30_b64_relu_relu_d0\n",
      "Testing -> na30_nb30_b64_relu_relu_d1\n",
      "Testing -> na30_nb30_b32_relu_linear_d0\n",
      "Testing -> na30_nb30_b32_relu_linear_d1\n",
      "Testing -> na30_nb30_b64_relu_linear_d0\n",
      "Testing -> na30_nb30_b64_relu_linear_d1\n",
      "Testing -> na30_nb30_b32_linear_relu_d0\n",
      "Testing -> na30_nb30_b32_linear_relu_d1\n",
      "Testing -> na30_nb30_b64_linear_relu_d0\n",
      "Testing -> na30_nb30_b64_linear_relu_d1\n",
      "Testing -> na30_nb30_b32_linear_linear_d0\n",
      "Testing -> na30_nb30_b32_linear_linear_d1\n",
      "Testing -> na30_nb30_b64_linear_linear_d0\n",
      "Testing -> na30_nb30_b64_linear_linear_d1\n",
      "na30_nb10_b64_relu_linear_d0 -> 0.8622520872012849\n",
      "na30_nb30_b32_linear_relu_d0 -> 0.8612748667022407\n",
      "na30_nb30_b64_linear_relu_d0 -> 0.8612302356915201\n",
      "na30_nb10_b64_relu_linear_d1 -> 0.8611270672302169\n",
      "na30_nb30_b64_linear_relu_d1 -> 0.8608383461718419\n",
      "na30_nb30_b32_linear_relu_d1 -> 0.8604025949319862\n",
      "na30_nb10_b64_relu_relu_d0 -> 0.8601135238116102\n",
      "na30_nb10_b32_linear_relu_d0 -> 0.8592332742755037\n",
      "na30_nb10_b64_linear_relu_d1 -> 0.8592082403928855\n",
      "na30_nb10_b64_relu_relu_d1 -> 0.8584326753429459\n"
     ]
    }
   ],
   "source": [
    "results_2 = {}\n",
    "neurona_1 = 30\n",
    "for neurona_2 in [10, 30]:\n",
    "    for act_n1 in ['relu', 'linear']:\n",
    "        for act_n2 in ['relu', 'linear']:\n",
    "            for batch in [32, 64]:\n",
    "                for drop in [0, 1]:\n",
    "                    name = 'na{}_nb{}_b{}_{}_{}_d{}'.format(neurona_1, neurona_2, batch, act_n1, act_n2, drop)\n",
    "                    print('Testing ->', name)\n",
    "                    # Armo el modelo\n",
    "                    test_model = Sequential(name=name)\n",
    "\n",
    "                    test_model.add(Dense(neurona_1, activation=act_n1, input_shape=(x_temp.shape[1],)))\n",
    "                    if drop:\n",
    "                        test_model.add(Dropout(rate=0.2))\n",
    "                    test_model.add(Dense(neurona_2, activation=act_n2))\n",
    "\n",
    "                    test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "                    # Compilo el modelo\n",
    "                    test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "                    auc, metrics = train_model(x_temp, y_temp, test_model, name, batch, stopping_patiece=50)\n",
    "                    results_2[name] = (auc, metrics)\n",
    "                \n",
    "sorted_results_2 = list(reversed(sorted(results_2.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results_2[0:10]:\n",
    "    print(i[0], '->', i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "810969db-cbe4-435b-ba8d-437521ea3ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "na30_nb30_b32_linear_relu_d0_b -> 0.8597472460015325\n",
      "na30_nb30_b32_linear_relu_d1_b -> 0.8597348839580329\n",
      "na30_nb10_b64_relu_linear_d1_b -> 0.8570038988289698\n",
      "na30_nb10_b64_relu_linear_d0_b -> 0.85498259413843\n"
     ]
    }
   ],
   "source": [
    "results_3 = {}\n",
    "redes = [[30, 10, 64, 'relu', 'linear'],\n",
    "         [30, 30, 32, 'linear', 'relu']]\n",
    "\n",
    "for n1, n2, b, a1, a2 in redes:\n",
    "    for drop in [0, 1]:\n",
    "        name = 'na{}_nb{}_b{}_{}_{}_d{}_b'.format(n1, n2, b, a1, a2, drop)\n",
    "        print('Testing ->', name)\n",
    "        # Armo el modelo\n",
    "        test_model = Sequential(name=name)\n",
    "\n",
    "        test_model.add(Dense(n1, activation=a1, input_shape=(x_temp.shape[1],)))\n",
    "        \n",
    "        test_model.add(Dense(n2, activation=a2))\n",
    "        \n",
    "        if drop:\n",
    "            test_model.add(Dropout(rate=0.5))\n",
    "        else:\n",
    "            test_model.add(BatchNormalization())\n",
    "            \n",
    "        test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "        # Compilo el modelo\n",
    "        test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "        auc, metrics = train_model(x_temp, y_temp, test_model, name, b, stopping_patiece=50)\n",
    "        results_3[name] = (auc, metrics)\n",
    "        clear_output(wait=True)\n",
    "                \n",
    "sorted_results_3 = list(reversed(sorted(results_3.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results_3:\n",
    "    print(i[0], '->', i[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256bac3-5304-454a-909f-1d8a7fd49676",
   "metadata": {},
   "source": [
    "Finalmente no se pudieron conseguir mejoras sustanciales agregando capas de Droput o BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825f34d-74bb-4b77-8f3f-2c5519e508cf",
   "metadata": {},
   "source": [
    "#### Optimizando hypperparametros del optimizador y regularizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dcb469e3-9b5d-49e2-8634-73c8e245344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red0_lr0.1_l20.01_ -> 0.8626701657288848\n",
      "red1_lr0.02_l20.25_ -> 0.8625874450119826\n",
      "red0_lr0.05_l20.01_ -> 0.8620204559421782\n",
      "red0_lr0.02_l20.05_ -> 0.8617906318241826\n",
      "red1_lr0.02_l20.1_ -> 0.8609841614011632\n",
      "red1_lr0.02_l20.05_ -> 0.8609068547063339\n",
      "red0_lr0.1_l20.5_ -> 0.8608401046760982\n",
      "red1_lr0.05_l20.25_ -> 0.8602072845659061\n",
      "red0_lr0.05_l20.1_ -> 0.8598520700543266\n",
      "red0_lr0.01_l20.25_ -> 0.8594192145576507\n"
     ]
    }
   ],
   "source": [
    "results_4 = {}\n",
    "redes = [[30, 10, 64, 'relu', 'linear'],\n",
    "         [30, 30, 32, 'linear', 'relu']]\n",
    "\n",
    "for i in [0, 1]:\n",
    "    n1, n2, b, a1, a2 = redes[i]\n",
    "    for lr in [5e-3, 0.01, 0.02, 0.05, 0.1, 0.5]:\n",
    "        for l_2 in [0.01, 0.05, 0.1, 0.25, 0.5]:\n",
    "            name = 'red{}_lr{}_l2{}_'.format(i, lr, l_2)\n",
    "            print('Testing ->', name)\n",
    "            # Armo el modelo\n",
    "            test_model = Sequential(name=name)\n",
    "\n",
    "            test_model.add(Dense(n1, activation=a1, input_shape=(x_temp.shape[1],)))\n",
    "\n",
    "            test_model.add(Dense(n2, activation=a2))\n",
    "\n",
    "            test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(l_2)))\n",
    "            # Compilo el modelo\n",
    "            test_model.compile(optimizer=Adam(learning_rate=lr), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "            auc, metrics = train_model(x_temp, y_temp, test_model, name, b, stopping_patiece=50)\n",
    "            results_4[name] = (auc, metrics)\n",
    "            clear_output(wait=True)\n",
    "                \n",
    "sorted_results_4 = list(reversed(sorted(results_4.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results_4[:10]:\n",
    "    print(i[0], '->', i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55624b90-ad70-42f6-9940-540b83ca4ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
