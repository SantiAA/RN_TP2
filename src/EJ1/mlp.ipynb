{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eadad356-bc8b-4ae3-af46-51624e9b6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from os.path import join\n",
    "from os import getcwd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0664c46-861a-429d-a736-14b3d54276ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, auc\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e9ea86-22bd-4c88-8937-c34f49ce6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import AUC # Area under the curve, default: ROC\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.initializers import GlorotNormal\n",
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fec708f-306d-4a12-9d9f-7f8020bb3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4822c966-b64f-4ca0-9604-c9cd4b875624",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = getcwd()+'\\\\checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a024e19-bfc5-45b0-b492-e57323f6183f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../databases/diabetes.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708d29ba-8b33-4740-909e-654070c43da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlayers = {\n",
    "    'BloodPressure': (40, np.Inf),\n",
    "    'SkinThickness': (0, 80),\n",
    "    'Insulin': (0, 400),\n",
    "    'BMI': (0, 50)\n",
    "}\n",
    "\n",
    "zeros = [\n",
    "    'Glucose',\n",
    "    'BloodPressure',\n",
    "    'SkinThickness',\n",
    "    'Insulin',\n",
    "    'BMI'\n",
    "]\n",
    "x_df = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
    "y_df = df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "077675fa-81fe-4dad-b1a2-af4a9384cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_changing_state(x_dataset, y_dataset, model, state, direction, my_callbacks):\n",
    "    # Split dataset into 15% test, 85% train \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_df, y_df, test_size=0.15, random_state=state)\n",
    "    # Reemplazo los valores nulos y los outlayer\n",
    "    x_train_clean, _data = replace_outliers_zeros(x_train, outlayers, zeros, mean_median=True)\n",
    "    x_valid_clean, _data = replace_outliers_zeros(x_valid, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "    # Normalizo los datasets\n",
    "    x_train_norm, _norm_dict = normalize(x_train_clean, None)\n",
    "    x_valid_norm, _norm_dict = normalize(x_valid_clean, _norm_dict)\n",
    "    # Train model\n",
    "    history_mlp_0 = mlp_model.fit(x_train_norm, y_train, validation_data=(x_valid_norm, y_valid),\n",
    "                              batch_size=32, epochs=200,\n",
    "                              verbose=0, callbacks=my_callbacks) \n",
    "    # Cargo el mejor modelo entrenado\n",
    "    mlp_model.load_weights(direction)\n",
    "    results = verify_model(mlp_model, x_train_norm, y_train, x_valid_norm, y_valid)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bab0fcf-e5f0-4578-a471-b4de4003fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into 15% test, 85% train \n",
    "x_temp, x_test, y_temp, y_test = train_test_split(x_df, y_df, test_size=0.15)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_temp, y_temp, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1928f7a-ec8c-45e1-b11f-ff653b4ab78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazo los valores nulos y los outlayer\n",
    "x_train_clean, _data = replace_outliers_zeros(x_train, outlayers, zeros, mean_median=True)\n",
    "x_test_clean, _data = replace_outliers_zeros(x_test, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "x_valid_clean, _data = replace_outliers_zeros(x_valid, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "# Normalizo los datasets\n",
    "x_train_norm, _norm_dict = normalize(x_train_clean, None)\n",
    "x_valid_norm, _norm_dict = normalize(x_valid_clean, _norm_dict)\n",
    "x_test_norm, _norm_dict = normalize(x_test_clean, _norm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc8e915-c772-4560-a6f5-434ecee7a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = EarlyStopping(monitor='val_auc', patience=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ce718-b28b-4d57-a0d7-3a99e8d02098",
   "metadata": {},
   "source": [
    "### Definici√≥n de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0135178c-9c0a-44c1-b1dc-4a3bfbd2f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlp_0\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_0_checkpoint_callback = ModelCheckpoint(filepath=join(checkpoints_path, 'mlp_0'), save_weights_only=True, monitor='val_auc', mode='max', \n",
    "                                           save_best_only=True)\n",
    "mlp_model = Sequential(name='mlp_0')\n",
    "\n",
    "mlp_model.add(Dense(10, activation='relu', input_shape=(x_train_norm.shape[1],)))\n",
    "mlp_model.add(Dense(5, activation='linear'))\n",
    "mlp_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5efacb4-e902-4b98-8aca-42089e2a1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c4e0371-ae2c-4392-9e57-c3ec22564c62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 1.0161 - auc: 0.7975 - val_loss: 0.7826 - val_auc: 0.8101\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 0.6250 - auc: 0.8532 - val_loss: 0.5886 - val_auc: 0.8206\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 19ms/step - loss: 0.5109 - auc: 0.8590 - val_loss: 0.5423 - val_auc: 0.8276\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4796 - auc: 0.8538 - val_loss: 0.5251 - val_auc: 0.8279\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8603 - val_loss: 0.5171 - val_auc: 0.8248\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 0.4524 - auc: 0.8633 - val_loss: 0.5185 - val_auc: 0.8310\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4504 - auc: 0.8635 - val_loss: 0.5093 - val_auc: 0.8243\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4495 - auc: 0.8646 - val_loss: 0.5199 - val_auc: 0.8288\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 0.4441 - auc: 0.8682 - val_loss: 0.4997 - val_auc: 0.8376\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4400 - auc: 0.8681 - val_loss: 0.5076 - val_auc: 0.8312\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4374 - auc: 0.8690 - val_loss: 0.4978 - val_auc: 0.8323\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4351 - auc: 0.8701 - val_loss: 0.4956 - val_auc: 0.8361\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4331 - auc: 0.8734 - val_loss: 0.5056 - val_auc: 0.8394\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4274 - auc: 0.8740 - val_loss: 0.4965 - val_auc: 0.8343\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4250 - auc: 0.8765 - val_loss: 0.5008 - val_auc: 0.8378\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4244 - auc: 0.8766 - val_loss: 0.4953 - val_auc: 0.8374\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4303 - auc: 0.8739 - val_loss: 0.5128 - val_auc: 0.8363\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4284 - auc: 0.8737 - val_loss: 0.5072 - val_auc: 0.8352\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4240 - auc: 0.8758 - val_loss: 0.4991 - val_auc: 0.8365\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4201 - auc: 0.8801 - val_loss: 0.4931 - val_auc: 0.8385\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.4199 - auc: 0.8790 - val_loss: 0.4891 - val_auc: 0.8434\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.4134 - auc: 0.8827 - val_loss: 0.4839 - val_auc: 0.8476\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4138 - auc: 0.8827 - val_loss: 0.4868 - val_auc: 0.8394\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4097 - auc: 0.8846 - val_loss: 0.4867 - val_auc: 0.8412\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4089 - auc: 0.8849 - val_loss: 0.4840 - val_auc: 0.8469\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4091 - auc: 0.8852 - val_loss: 0.4833 - val_auc: 0.8467\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4099 - auc: 0.8833 - val_loss: 0.4861 - val_auc: 0.8458\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.4086 - auc: 0.8864 - val_loss: 0.4819 - val_auc: 0.8502\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 0.4059 - auc: 0.8864 - val_loss: 0.4779 - val_auc: 0.8511\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4036 - auc: 0.8879 - val_loss: 0.4777 - val_auc: 0.8487\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4054 - auc: 0.8866 - val_loss: 0.4936 - val_auc: 0.8454\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4071 - auc: 0.8851 - val_loss: 0.4950 - val_auc: 0.8456\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3982 - auc: 0.8910 - val_loss: 0.4826 - val_auc: 0.8480\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4030 - auc: 0.8870 - val_loss: 0.4789 - val_auc: 0.8498\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.4092 - auc: 0.8850 - val_loss: 0.4772 - val_auc: 0.8520\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3981 - auc: 0.8912 - val_loss: 0.4912 - val_auc: 0.8432\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3991 - auc: 0.8909 - val_loss: 0.4848 - val_auc: 0.8463\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3975 - auc: 0.8917 - val_loss: 0.4967 - val_auc: 0.8447\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3946 - auc: 0.8933 - val_loss: 0.4896 - val_auc: 0.8425\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3960 - auc: 0.8919 - val_loss: 0.4911 - val_auc: 0.8425\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3944 - auc: 0.8928 - val_loss: 0.4879 - val_auc: 0.8471\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3976 - auc: 0.8911 - val_loss: 0.4861 - val_auc: 0.8387\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3941 - auc: 0.8928 - val_loss: 0.4906 - val_auc: 0.8401\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3915 - auc: 0.8952 - val_loss: 0.4963 - val_auc: 0.8416\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3926 - auc: 0.8944 - val_loss: 0.5114 - val_auc: 0.8330\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3942 - auc: 0.8926 - val_loss: 0.4963 - val_auc: 0.8381\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3915 - auc: 0.8948 - val_loss: 0.4890 - val_auc: 0.8394\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3932 - auc: 0.8941 - val_loss: 0.4981 - val_auc: 0.8409\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3945 - auc: 0.8932 - val_loss: 0.5187 - val_auc: 0.8443\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4006 - auc: 0.8905 - val_loss: 0.4892 - val_auc: 0.8403\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3910 - auc: 0.8958 - val_loss: 0.4865 - val_auc: 0.8429\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3929 - auc: 0.8944 - val_loss: 0.4888 - val_auc: 0.8376\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4073 - auc: 0.8859 - val_loss: 0.4912 - val_auc: 0.8427\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4019 - auc: 0.8921 - val_loss: 0.5251 - val_auc: 0.8274\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4003 - auc: 0.8917 - val_loss: 0.5310 - val_auc: 0.8252\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3950 - auc: 0.8946 - val_loss: 0.4944 - val_auc: 0.8327\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3918 - auc: 0.8945 - val_loss: 0.4936 - val_auc: 0.8385\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3859 - auc: 0.8981 - val_loss: 0.4859 - val_auc: 0.8438\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3885 - auc: 0.8993 - val_loss: 0.4872 - val_auc: 0.8332\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4159 - auc: 0.8863 - val_loss: 0.5238 - val_auc: 0.8412\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3975 - auc: 0.8943 - val_loss: 0.5047 - val_auc: 0.8341\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3891 - auc: 0.8975 - val_loss: 0.4955 - val_auc: 0.8296\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3891 - auc: 0.8963 - val_loss: 0.5030 - val_auc: 0.8365\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3889 - auc: 0.8971 - val_loss: 0.4812 - val_auc: 0.8429\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3923 - auc: 0.8955 - val_loss: 0.4987 - val_auc: 0.8396\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3856 - auc: 0.8985 - val_loss: 0.5061 - val_auc: 0.8345\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3838 - auc: 0.8998 - val_loss: 0.4988 - val_auc: 0.8383\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3849 - auc: 0.8978 - val_loss: 0.5186 - val_auc: 0.8418\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3977 - auc: 0.8940 - val_loss: 0.4872 - val_auc: 0.8423\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3889 - auc: 0.8971 - val_loss: 0.4972 - val_auc: 0.8323\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3852 - auc: 0.8989 - val_loss: 0.4914 - val_auc: 0.8347\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3825 - auc: 0.8996 - val_loss: 0.4966 - val_auc: 0.8372\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3817 - auc: 0.9007 - val_loss: 0.4984 - val_auc: 0.8252\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3853 - auc: 0.8994 - val_loss: 0.4919 - val_auc: 0.8334\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3837 - auc: 0.8988 - val_loss: 0.4864 - val_auc: 0.8396\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3788 - auc: 0.9021 - val_loss: 0.4969 - val_auc: 0.8398\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3859 - auc: 0.8989 - val_loss: 0.4898 - val_auc: 0.8288\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3872 - auc: 0.8984 - val_loss: 0.4975 - val_auc: 0.8339\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3862 - auc: 0.8981 - val_loss: 0.5182 - val_auc: 0.8270\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3815 - auc: 0.9018 - val_loss: 0.4939 - val_auc: 0.8358\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3853 - auc: 0.8983 - val_loss: 0.4907 - val_auc: 0.8257\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3800 - auc: 0.9015 - val_loss: 0.4882 - val_auc: 0.8367\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3805 - auc: 0.9008 - val_loss: 0.4943 - val_auc: 0.8334\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3796 - auc: 0.9018 - val_loss: 0.4884 - val_auc: 0.8314\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3826 - auc: 0.8999 - val_loss: 0.5315 - val_auc: 0.8314\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3851 - auc: 0.8993 - val_loss: 0.5180 - val_auc: 0.8292\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3824 - auc: 0.9027 - val_loss: 0.4886 - val_auc: 0.8268\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3856 - auc: 0.8989 - val_loss: 0.4962 - val_auc: 0.8261\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3801 - auc: 0.9014 - val_loss: 0.4989 - val_auc: 0.8321\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3815 - auc: 0.9017 - val_loss: 0.4890 - val_auc: 0.8341\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3805 - auc: 0.9005 - val_loss: 0.5747 - val_auc: 0.8230\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3830 - auc: 0.9017 - val_loss: 0.4830 - val_auc: 0.8325\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3817 - auc: 0.9019 - val_loss: 0.5011 - val_auc: 0.8208\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3781 - auc: 0.9021 - val_loss: 0.4874 - val_auc: 0.8259\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3891 - auc: 0.8976 - val_loss: 0.5040 - val_auc: 0.8272\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3853 - auc: 0.8990 - val_loss: 0.5075 - val_auc: 0.8230\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3817 - auc: 0.9001 - val_loss: 0.4869 - val_auc: 0.8339\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3721 - auc: 0.9045 - val_loss: 0.5121 - val_auc: 0.8321\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3797 - auc: 0.9034 - val_loss: 0.4896 - val_auc: 0.8237\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3804 - auc: 0.9013 - val_loss: 0.5080 - val_auc: 0.8248\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3850 - auc: 0.8997 - val_loss: 0.5308 - val_auc: 0.8336\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3760 - auc: 0.9038 - val_loss: 0.4906 - val_auc: 0.8294\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3784 - auc: 0.9028 - val_loss: 0.4927 - val_auc: 0.8272\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3751 - auc: 0.9042 - val_loss: 0.4822 - val_auc: 0.8343\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3826 - auc: 0.8997 - val_loss: 0.5015 - val_auc: 0.8279\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3850 - auc: 0.9014 - val_loss: 0.5095 - val_auc: 0.8252\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3753 - auc: 0.9045 - val_loss: 0.4983 - val_auc: 0.8232\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3782 - auc: 0.9031 - val_loss: 0.4932 - val_auc: 0.8199\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3730 - auc: 0.9059 - val_loss: 0.5254 - val_auc: 0.8243\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3771 - auc: 0.9023 - val_loss: 0.5125 - val_auc: 0.8265\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3748 - auc: 0.9031 - val_loss: 0.4939 - val_auc: 0.8310\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3737 - auc: 0.9044 - val_loss: 0.4903 - val_auc: 0.8327\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3754 - auc: 0.9039 - val_loss: 0.5124 - val_auc: 0.8232\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3775 - auc: 0.9039 - val_loss: 0.5073 - val_auc: 0.8234\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3856 - auc: 0.8998 - val_loss: 0.4987 - val_auc: 0.8245\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3756 - auc: 0.9043 - val_loss: 0.4978 - val_auc: 0.8237\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3724 - auc: 0.9062 - val_loss: 0.5044 - val_auc: 0.8319\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3706 - auc: 0.9057 - val_loss: 0.4989 - val_auc: 0.8259\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3720 - auc: 0.9058 - val_loss: 0.4956 - val_auc: 0.8217\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3774 - auc: 0.9032 - val_loss: 0.4971 - val_auc: 0.8281\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3760 - auc: 0.9043 - val_loss: 0.4973 - val_auc: 0.8265\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3794 - auc: 0.9026 - val_loss: 0.5035 - val_auc: 0.8283\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3729 - auc: 0.9050 - val_loss: 0.5167 - val_auc: 0.8241\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3700 - auc: 0.9075 - val_loss: 0.4875 - val_auc: 0.8234\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3709 - auc: 0.9088 - val_loss: 0.5182 - val_auc: 0.8250\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3755 - auc: 0.9040 - val_loss: 0.5190 - val_auc: 0.8270\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3727 - auc: 0.9041 - val_loss: 0.5000 - val_auc: 0.8239\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3820 - auc: 0.9029 - val_loss: 0.4924 - val_auc: 0.8206\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3868 - auc: 0.8992 - val_loss: 0.4943 - val_auc: 0.8248\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3868 - auc: 0.9014 - val_loss: 0.5491 - val_auc: 0.8197\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3684 - auc: 0.9074 - val_loss: 0.5038 - val_auc: 0.8217\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3722 - auc: 0.9061 - val_loss: 0.4932 - val_auc: 0.8265\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3797 - auc: 0.9043 - val_loss: 0.5323 - val_auc: 0.8259\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3683 - auc: 0.9090 - val_loss: 0.5268 - val_auc: 0.8230\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3695 - auc: 0.9074 - val_loss: 0.5314 - val_auc: 0.8179\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3701 - auc: 0.9073 - val_loss: 0.5174 - val_auc: 0.8274\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3673 - auc: 0.9090 - val_loss: 0.4946 - val_auc: 0.8228\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3687 - auc: 0.9089 - val_loss: 0.5511 - val_auc: 0.8203\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3724 - auc: 0.9072 - val_loss: 0.5729 - val_auc: 0.8172\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3871 - auc: 0.9018 - val_loss: 0.5275 - val_auc: 0.8137\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3784 - auc: 0.9080 - val_loss: 0.4878 - val_auc: 0.8239\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3701 - auc: 0.9086 - val_loss: 0.5766 - val_auc: 0.8095\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3730 - auc: 0.9068 - val_loss: 0.5216 - val_auc: 0.8150\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3636 - auc: 0.9117 - val_loss: 0.5102 - val_auc: 0.8152\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3654 - auc: 0.9111 - val_loss: 0.5671 - val_auc: 0.8144\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3808 - auc: 0.9026 - val_loss: 0.5097 - val_auc: 0.8121\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3660 - auc: 0.9114 - val_loss: 0.5151 - val_auc: 0.8179\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3681 - auc: 0.9114 - val_loss: 0.5733 - val_auc: 0.8210\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3703 - auc: 0.9094 - val_loss: 0.5261 - val_auc: 0.8210\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3640 - auc: 0.9122 - val_loss: 0.5207 - val_auc: 0.8157\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3700 - auc: 0.9109 - val_loss: 0.5191 - val_auc: 0.8015\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3753 - auc: 0.9068 - val_loss: 0.5738 - val_auc: 0.8104\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3651 - auc: 0.9113 - val_loss: 0.5312 - val_auc: 0.8130\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3642 - auc: 0.9118 - val_loss: 0.5197 - val_auc: 0.8168\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3641 - auc: 0.9110 - val_loss: 0.5172 - val_auc: 0.8170\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3596 - auc: 0.9137 - val_loss: 0.5269 - val_auc: 0.8195\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3609 - auc: 0.9118 - val_loss: 0.5246 - val_auc: 0.8166\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3586 - auc: 0.9132 - val_loss: 0.5253 - val_auc: 0.8126\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3599 - auc: 0.9130 - val_loss: 0.5280 - val_auc: 0.8155\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3564 - auc: 0.9143 - val_loss: 0.5279 - val_auc: 0.8104\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3656 - auc: 0.9098 - val_loss: 0.5240 - val_auc: 0.8042\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3640 - auc: 0.9122 - val_loss: 0.5711 - val_auc: 0.8132\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3697 - auc: 0.9104 - val_loss: 0.5253 - val_auc: 0.8084\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3641 - auc: 0.9114 - val_loss: 0.5238 - val_auc: 0.8082\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3697 - auc: 0.9123 - val_loss: 0.5074 - val_auc: 0.8108\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3618 - auc: 0.9154 - val_loss: 0.5753 - val_auc: 0.8137\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3688 - auc: 0.9098 - val_loss: 0.5951 - val_auc: 0.8059\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3761 - auc: 0.9092 - val_loss: 0.5241 - val_auc: 0.7946\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3728 - auc: 0.9105 - val_loss: 0.5679 - val_auc: 0.7984\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3650 - auc: 0.9144 - val_loss: 0.5647 - val_auc: 0.8077\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3648 - auc: 0.9130 - val_loss: 0.5582 - val_auc: 0.8064\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3795 - auc: 0.9097 - val_loss: 0.5264 - val_auc: 0.7944\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3813 - auc: 0.9074 - val_loss: 0.5772 - val_auc: 0.7977\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3660 - auc: 0.9151 - val_loss: 0.5850 - val_auc: 0.8106\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3710 - auc: 0.9116 - val_loss: 0.5358 - val_auc: 0.8106\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3623 - auc: 0.9133 - val_loss: 0.5194 - val_auc: 0.8042\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3674 - auc: 0.9100 - val_loss: 0.5269 - val_auc: 0.7991\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3613 - auc: 0.9137 - val_loss: 0.5535 - val_auc: 0.7982\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4813 - auc: 0.850 - 0s 2ms/step - loss: 0.3575 - auc: 0.9148 - val_loss: 0.5507 - val_auc: 0.7997\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3574 - auc: 0.9161 - val_loss: 0.5302 - val_auc: 0.7946\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3672 - auc: 0.9132 - val_loss: 0.5450 - val_auc: 0.7984\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3725 - auc: 0.9086 - val_loss: 0.6705 - val_auc: 0.8024\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3706 - auc: 0.9097 - val_loss: 0.5341 - val_auc: 0.7995\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3585 - auc: 0.9175 - val_loss: 0.5577 - val_auc: 0.8006\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3689 - auc: 0.9102 - val_loss: 0.5410 - val_auc: 0.8066\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3609 - auc: 0.9131 - val_loss: 0.5898 - val_auc: 0.8017\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3634 - auc: 0.9127 - val_loss: 0.5619 - val_auc: 0.7949\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3546 - auc: 0.9156 - val_loss: 0.5661 - val_auc: 0.7995\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3538 - auc: 0.9157 - val_loss: 0.5512 - val_auc: 0.7982\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3544 - auc: 0.9156 - val_loss: 0.5746 - val_auc: 0.8053\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3563 - auc: 0.9141 - val_loss: 0.5555 - val_auc: 0.7982\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3532 - auc: 0.9153 - val_loss: 0.5824 - val_auc: 0.7960\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3647 - auc: 0.9118 - val_loss: 0.5481 - val_auc: 0.8013\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3560 - auc: 0.9163 - val_loss: 0.5480 - val_auc: 0.7869\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3573 - auc: 0.9175 - val_loss: 0.5612 - val_auc: 0.7940\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3551 - auc: 0.9175 - val_loss: 0.6346 - val_auc: 0.7922\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3589 - auc: 0.9135 - val_loss: 0.5586 - val_auc: 0.7940\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3572 - auc: 0.9135 - val_loss: 0.5358 - val_auc: 0.8006\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3605 - auc: 0.9148 - val_loss: 0.6142 - val_auc: 0.7833\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3570 - auc: 0.9141 - val_loss: 0.5668 - val_auc: 0.7962\n"
     ]
    }
   ],
   "source": [
    "history_mlp_0 = mlp_model.fit(x_train_norm, y_train, validation_data=(x_valid_norm, y_valid),\n",
    "                              batch_size=32, epochs=200,\n",
    "                              verbose=1, callbacks=[mlp_0_checkpoint_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2468623-f729-4536-b0aa-0376dec4f190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set</th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Especificidad</th>\n",
       "      <th>Sensibilidad</th>\n",
       "      <th>Valor Predictivo Positivo</th>\n",
       "      <th>Valor Predictivo Negativo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>0.895030</td>\n",
       "      <td>0.871935</td>\n",
       "      <td>0.695187</td>\n",
       "      <td>0.734463</td>\n",
       "      <td>0.848806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Validacion</td>\n",
       "      <td>0.851573</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.815385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Set   AUC ROC  Especificidad  Sensibilidad  \\\n",
       "0       Train  0.895030       0.871935      0.695187   \n",
       "1  Validacion  0.851573       0.868852      0.675676   \n",
       "\n",
       "   Valor Predictivo Positivo  Valor Predictivo Negativo  \n",
       "0                   0.734463                   0.848806  \n",
       "1                   0.757576                   0.815385  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargo el mejor modelo entrenado\n",
    "mlp_model.load_weights(join(checkpoints_path, 'mlp_0'))\n",
    "verify_model(mlp_model, x_train_norm, y_train, x_valid_norm, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d906e-cc88-413d-a538-acf90b7fc195",
   "metadata": {},
   "source": [
    "Salieron resultados interesantes, vamos a verificar si se mantiene al cambiar el random state al hacer split de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e491747-9265-4e72-9cbe-de10788cbfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlp_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_state = Sequential(name='mlp_1')\n",
    "mlp_state.add(Dense(10, activation='relu', input_shape=(x_train_norm.shape[1],)))\n",
    "mlp_state.add(Dense(5, activation='linear'))\n",
    "mlp_state.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "mlp_state.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "pesos_inicial = mlp_state.get_weights()\n",
    "mlp_state.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c79987f8-7c3f-4504-9e03-1bfed5c67d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing state 0\n",
      "Runing state 1\n",
      "Runing state 2\n",
      "Runing state 3\n",
      "Runing state 4\n",
      "Runing state 5\n",
      "Runing state 6\n",
      "Runing state 7\n",
      "Runing state 8\n",
      "Runing state 9\n"
     ]
    }
   ],
   "source": [
    "train_auc = []\n",
    "valid_auc = []\n",
    "results = []\n",
    "\n",
    "for i in range(10):\n",
    "    print('Runing state', i)\n",
    "    name  = 'mlp_state_{}'.format(i)\n",
    "    mlp_state.set_weights(pesos_inicial)\n",
    "    checkdir = join(checkpoints_path, name)\n",
    "    temp_callback = ModelCheckpoint(filepath=checkdir, save_weights_only=True, monitor='val_auc', mode='max', save_best_only=True)\n",
    "    metrics = run_changing_state(x_temp, y_temp, mlp_state, i, checkdir, [temp_callback])\n",
    "    results.append(metrics)\n",
    "    train_auc.append(metrics['AUC ROC'][0])\n",
    "    valid_auc.append(metrics['AUC ROC'][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cdace0c-54bb-411b-93c1-4c1111d267d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_auc, label='Train')\n",
    "plt.plot(valid_auc, label='Validation')\n",
    "mean = np.mean(valid_auc)\n",
    "std = np.std(valid_auc)\n",
    "plt.hlines([mean+std, mean, mean-std], 0, 9)   \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc3079-2d04-4d44-b345-c8a8be95bf84",
   "metadata": {},
   "source": [
    "Se puede observar que en la mayoria de los casos la metrica de validaci√≥n da mejor que la metrica de train, con lo cual se puede inferir que el modelo tendr√° una gran varianza debido a la poca cantidad de datos disponible para los par√°metros que requiere entrenar el modelo. Por lo tanto una mejor opcion es utilizar k-folding en lugar de hold out validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901419e-6962-4b1b-9ed4-a1a2acfa0e54",
   "metadata": {},
   "source": [
    "Ahora vamos a probar utilizar 1 o 2 capas ocultas, variando la cantidad de neuronas entre 5, 10 y 30, modificando la activacion entre relu y lineal. Para ello utilizaremos la tecnica de k-folding y la metrica de cada set de hyperpar√°metros sera el promedio de las metricas de cada fold. Una vez testeado esto se continuara buscando mejorar el modelo cambiando la regularizacion o agregando capas de Droput o Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32deaee6-b847-4210-9f3d-944edec57fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folding = KFold(n_splits=5)\n",
    "\n",
    "def train_model(x_dataset, y_dataset, model, name, batch, stopping_patiece=None):\n",
    "    pesos_default = model.get_weights()\n",
    "    folds = folding.split(x_dataset)\n",
    "    auc_val = []\n",
    "    results = []\n",
    "    curr = 0\n",
    "    for train, valid in folds:\n",
    "        # Split dataset\n",
    "        x_train, x_valid = x_dataset.iloc[train], x_dataset.iloc[valid]\n",
    "        y_train, y_valid = y_dataset.iloc[train], y_dataset.iloc[valid]\n",
    "        \n",
    "        # Proceso los datos\n",
    "        x_train_c, _data = replace_outliers_zeros(x_train, outlayers, zeros, mean_median=True)\n",
    "        x_valid_c, _data = replace_outliers_zeros(x_valid, outlayers, zeros, mean_median=True, data_to_replace=_data)\n",
    "        # Normalizo los datasets\n",
    "        x_train_n, _norm_dict = normalize(x_train_c, None)\n",
    "        x_valid_n, _norm_dict = normalize(x_valid_c, _norm_dict)\n",
    "    \n",
    "        # Create callbacks\n",
    "        checkdir = join(checkpoints_path, name+'f{}'.format(curr))\n",
    "        temp_callback = ModelCheckpoint(filepath=checkdir, save_weights_only=True, monitor='val_auc', mode='max', save_best_only=True)\n",
    "        my_callbacks = [temp_callback]\n",
    "        if stopping_patiece:\n",
    "            my_callbacks.append(EarlyStopping(monitor='val_auc', patience=stopping_patiece))\n",
    "        # Train model\n",
    "        history = model.fit(x_train_n, y_train, validation_data=(x_valid_n, y_valid),\n",
    "                            batch_size=batch, epochs=200,\n",
    "                            verbose=0, callbacks=my_callbacks) \n",
    "        # Cargo el mejor modelo entrenado\n",
    "        model.load_weights(checkdir)\n",
    "        metrics = verify_model(model, x_train_n, y_train, x_valid_n, y_valid)\n",
    "        auc_val.append(metrics['AUC ROC'][1])\n",
    "        results.append(metrics)\n",
    "        # Reset model for next training\n",
    "        model.set_weights(pesos_default)\n",
    "        curr += 1\n",
    "    return np.mean(auc_val), results    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2080a7-310c-4822-8a1f-ed6c835c5dc3",
   "metadata": {},
   "source": [
    "1 capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e8759-b00f-4a5b-88c2-7606855946a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing -> n5_b32_linear\n",
      "Auc score =  0.8428004696263288\n",
      "Testing -> n5_b64_linear\n",
      "Auc score =  0.8430584577052505\n",
      "Testing -> n5_b32_relu\n",
      "Auc score =  0.8385705047092109\n",
      "Testing -> n5_b64_relu\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for neuronas in [5, 10, 30]:\n",
    "    for activation in ['linear', 'relu']:\n",
    "        for batch in [32, 64]:\n",
    "            name = 'n{}_b{}_{}'.format(neuronas, batch, activation)\n",
    "            print('Testing ->', name)\n",
    "            # Armo el modelo\n",
    "            test_model = Sequential(name=name)\n",
    "            test_model.add(Dense(neuronas, activation=activation, input_shape=(x_temp.shape[1],)))\n",
    "            test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "            # Compilo el modelo\n",
    "            test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "            auc, metrics = train_model(x_temp, y_temp, test_model, name, batch, stopping_patiece=50)\n",
    "            results[name] = (auc, metrics)\n",
    "            print('Auc score = ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522c119-5a74-4aaf-b315-63453017dd68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for neuronas_1 in [5, 10, 30]:\n",
    "    for neuronas_2 in [5, 10, 30]:\n",
    "        for activation in ['linear', 'relu']:\n",
    "            for batch in [32, 64]:\n",
    "                name = 'na{}_nb{}_b{}_{}'.format(neuronas_1, neuronas_2, batch, activation)\n",
    "                print('Testing ->', name)\n",
    "                # Armo el modelo\n",
    "                test_model = Sequential(name=name)\n",
    "                test_model.add(Dense(neuronas_1, activation=activation, input_shape=(x_temp.shape[1],)))\n",
    "                test_model.add(Dense(neuronas_2, activation=activation))\n",
    "                test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "                # Compilo el modelo\n",
    "                test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "                auc, metrics = train_model(x_temp, y_temp, test_model, name, batch, stopping_patiece=50)\n",
    "                results[name] = (auc, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe383c-2854-4ae0-8838-738747eedd59",
   "metadata": {},
   "source": [
    "Veo los 10 mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b488f9-fcef-4761-b17c-a38956166f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_results = list(reversed(sorted(results.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results[0:10]:\n",
    "    print(i[0], '->', i[1][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c6fbf-5ebd-4482-84d8-8760d599e0e5",
   "metadata": {},
   "source": [
    "Se puede observar como los de mejor performance son en su mayoria redes con 2 capas ocultas, y con gran cantidad de neuronas en cada una. Con estos resultados se buscara optimizar los hyperpar√°metros de los 3 mejores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a31e61-2ea6-4415-807f-312dd67f09ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_2 = {}\n",
    "neurona_1 = 30\n",
    "for neurona_2 in [10, 30]:\n",
    "    for act_n1 in ['relu', 'linear']:\n",
    "        for act_n2 in ['relu', 'linear']:\n",
    "            for batch in [32, 64]:\n",
    "                for drop in [0, 1]:\n",
    "                    name = 'na{}_nb{}_b{}_{}_{}_d{}'.format(neurona_1, neurona_2, batch, act_n1, act_n2, drop)\n",
    "                    print('Testing ->', name)\n",
    "                    # Armo el modelo\n",
    "                    test_model = Sequential(name=name)\n",
    "\n",
    "                    test_model.add(Dense(neurona_1, activation=act_n1, input_shape=(x_temp.shape[1],)))\n",
    "                    if drop:\n",
    "                        test_model.add(Dropout(rate=0.2))\n",
    "                    test_model.add(Dense(neurona_2, activation=act_n2))\n",
    "\n",
    "                    test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "                    # Compilo el modelo\n",
    "                    test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "                    auc, metrics = train_model(x_temp, y_temp, test_model, name, batch, stopping_patiece=50)\n",
    "                    results_2[name] = (auc, metrics)\n",
    "                \n",
    "sorted_results_2 = list(reversed(sorted(results_2.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results_2[0:10]:\n",
    "    print(i[0], '->', i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810969db-cbe4-435b-ba8d-437521ea3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3 = {}\n",
    "redes = [[30, 10, 64, 'relu', 'linear'],\n",
    "         [30, 30, 32, 'linear', 'relu']]\n",
    "\n",
    "for n1, n2, b, a1, a2 in redes:\n",
    "    for drop in [0, 1]:\n",
    "        name = 'na{}_nb{}_b{}_{}_{}_d{}_b'.format(n1, n2, b, a1, a2, drop)\n",
    "        print('Testing ->', name)\n",
    "        # Armo el modelo\n",
    "        test_model = Sequential(name=name)\n",
    "\n",
    "        test_model.add(Dense(n1, activation=a1, input_shape=(x_temp.shape[1],)))\n",
    "        \n",
    "        test_model.add(Dense(n2, activation=a2))\n",
    "        \n",
    "        if drop:\n",
    "            test_model.add(Dropout(rate=0.5))\n",
    "        else:\n",
    "            test_model.add(BatchNormalization())\n",
    "            \n",
    "        test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.25)))\n",
    "        # Compilo el modelo\n",
    "        test_model.compile(optimizer=Adam(learning_rate=0.02), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "        auc, metrics = train_model(x_temp, y_temp, test_model, name, b, stopping_patiece=50)\n",
    "        results_3[name] = (auc, metrics)\n",
    "        clear_output(wait=True)\n",
    "                \n",
    "sorted_results_3 = list(reversed(sorted(results_3.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results_3:\n",
    "    print(i[0], '->', i[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256bac3-5304-454a-909f-1d8a7fd49676",
   "metadata": {},
   "source": [
    "Finalmente no se pudieron conseguir mejoras sustanciales agregando capas de Droput o BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825f34d-74bb-4b77-8f3f-2c5519e508cf",
   "metadata": {},
   "source": [
    "#### Optimizando hypperparametros del optimizador y regularizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb469e3-9b5d-49e2-8634-73c8e245344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_4 = {}\n",
    "redes = [[30, 10, 64, 'relu', 'linear'],\n",
    "         [30, 30, 32, 'linear', 'relu']]\n",
    "\n",
    "for i in [0, 1]:\n",
    "    n1, n2, b, a1, a2 = redes[i]\n",
    "    for lr in [5e-3, 0.01, 0.02, 0.05, 0.1, 0.5]:\n",
    "        for l_2 in [0.01, 0.05, 0.1, 0.25, 0.5]:\n",
    "            name = 'red{}_lr{}_l2{}_'.format(i, lr, l_2)\n",
    "            print('Testing ->', name)\n",
    "            # Armo el modelo\n",
    "            test_model = Sequential(name=name)\n",
    "\n",
    "            test_model.add(Dense(n1, activation=a1, input_shape=(x_temp.shape[1],)))\n",
    "\n",
    "            test_model.add(Dense(n2, activation=a2))\n",
    "\n",
    "            test_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(l_2)))\n",
    "            # Compilo el modelo\n",
    "            test_model.compile(optimizer=Adam(learning_rate=lr), loss=BinaryCrossentropy(), metrics=[AUC(name='auc')])\n",
    "            auc, metrics = train_model(x_temp, y_temp, test_model, name, b, stopping_patiece=50)\n",
    "            results_4[name] = (auc, metrics)\n",
    "            clear_output(wait=True)\n",
    "                \n",
    "sorted_results_4 = list(reversed(sorted(results_4.items(), key=lambda x: x[1][0])))\n",
    "for i in sorted_results_4[:10]:\n",
    "    print(i[0], '->', i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55624b90-ad70-42f6-9940-540b83ca4ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
